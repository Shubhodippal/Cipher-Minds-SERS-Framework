{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T17:06:13.097003Z",
     "iopub.status.busy": "2024-08-14T17:06:13.096672Z",
     "iopub.status.idle": "2024-08-14T17:06:26.950459Z",
     "shell.execute_reply": "2024-08-14T17:06:26.949613Z",
     "shell.execute_reply.started": "2024-08-14T17:06:13.096976Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# librosa is a Python library for analyzing audio and music. It can be used to extract the data from the audio files we will see it later.\n",
    "import librosa\n",
    "import librosa.display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# to play the audio files\n",
    "from IPython.display import Audio\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, BatchNormalization, MaxPooling1D, Dropout, GlobalAveragePooling1D, Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T17:06:26.952927Z",
     "iopub.status.busy": "2024-08-14T17:06:26.952363Z",
     "iopub.status.idle": "2024-08-14T17:06:28.955103Z",
     "shell.execute_reply": "2024-08-14T17:06:28.954110Z",
     "shell.execute_reply.started": "2024-08-14T17:06:26.952899Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>Emotion_Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.181929</td>\n",
       "      <td>0.597162</td>\n",
       "      <td>0.584452</td>\n",
       "      <td>0.663637</td>\n",
       "      <td>0.738467</td>\n",
       "      <td>0.684251</td>\n",
       "      <td>0.592274</td>\n",
       "      <td>0.605217</td>\n",
       "      <td>0.611734</td>\n",
       "      <td>0.713361</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001424</td>\n",
       "      <td>0.000824</td>\n",
       "      <td>2.245705e-03</td>\n",
       "      <td>4.730351e-03</td>\n",
       "      <td>7.843456e-03</td>\n",
       "      <td>6.678044e-03</td>\n",
       "      <td>2.589605e-03</td>\n",
       "      <td>1.258858e-03</td>\n",
       "      <td>3.716425e-05</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.373793</td>\n",
       "      <td>0.752836</td>\n",
       "      <td>0.767736</td>\n",
       "      <td>0.811567</td>\n",
       "      <td>0.837361</td>\n",
       "      <td>0.801382</td>\n",
       "      <td>0.736421</td>\n",
       "      <td>0.722412</td>\n",
       "      <td>0.721140</td>\n",
       "      <td>0.749309</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002378</td>\n",
       "      <td>0.001764</td>\n",
       "      <td>3.191589e-03</td>\n",
       "      <td>5.683203e-03</td>\n",
       "      <td>8.739583e-03</td>\n",
       "      <td>7.548534e-03</td>\n",
       "      <td>3.495810e-03</td>\n",
       "      <td>2.288241e-03</td>\n",
       "      <td>1.007357e-03</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.179078</td>\n",
       "      <td>0.581407</td>\n",
       "      <td>0.576625</td>\n",
       "      <td>0.674005</td>\n",
       "      <td>0.743081</td>\n",
       "      <td>0.647708</td>\n",
       "      <td>0.559811</td>\n",
       "      <td>0.605573</td>\n",
       "      <td>0.612860</td>\n",
       "      <td>0.715582</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000581</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>8.819485e-04</td>\n",
       "      <td>1.978945e-03</td>\n",
       "      <td>3.881180e-03</td>\n",
       "      <td>3.024464e-03</td>\n",
       "      <td>9.673847e-04</td>\n",
       "      <td>6.371708e-04</td>\n",
       "      <td>1.669109e-05</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.181940</td>\n",
       "      <td>0.609392</td>\n",
       "      <td>0.576482</td>\n",
       "      <td>0.649354</td>\n",
       "      <td>0.732143</td>\n",
       "      <td>0.703664</td>\n",
       "      <td>0.598831</td>\n",
       "      <td>0.607627</td>\n",
       "      <td>0.609744</td>\n",
       "      <td>0.704340</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001424</td>\n",
       "      <td>0.000824</td>\n",
       "      <td>2.245646e-03</td>\n",
       "      <td>4.730394e-03</td>\n",
       "      <td>7.843437e-03</td>\n",
       "      <td>6.678062e-03</td>\n",
       "      <td>2.589596e-03</td>\n",
       "      <td>1.258859e-03</td>\n",
       "      <td>3.716428e-05</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.430646</td>\n",
       "      <td>0.690136</td>\n",
       "      <td>0.681142</td>\n",
       "      <td>0.712062</td>\n",
       "      <td>0.746599</td>\n",
       "      <td>0.718833</td>\n",
       "      <td>0.536348</td>\n",
       "      <td>0.523392</td>\n",
       "      <td>0.554430</td>\n",
       "      <td>0.522560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006053</td>\n",
       "      <td>0.014728</td>\n",
       "      <td>2.603143e-02</td>\n",
       "      <td>2.871043e-02</td>\n",
       "      <td>8.294137e-03</td>\n",
       "      <td>7.164728e-03</td>\n",
       "      <td>2.395335e-03</td>\n",
       "      <td>6.251118e-04</td>\n",
       "      <td>1.206434e-05</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5863</th>\n",
       "      <td>0.050528</td>\n",
       "      <td>0.671133</td>\n",
       "      <td>0.653239</td>\n",
       "      <td>0.666003</td>\n",
       "      <td>0.645347</td>\n",
       "      <td>0.701951</td>\n",
       "      <td>0.763060</td>\n",
       "      <td>0.787043</td>\n",
       "      <td>0.806889</td>\n",
       "      <td>0.784755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>7.980836e-07</td>\n",
       "      <td>9.887120e-07</td>\n",
       "      <td>8.714226e-07</td>\n",
       "      <td>7.584952e-07</td>\n",
       "      <td>8.125273e-07</td>\n",
       "      <td>4.392478e-07</td>\n",
       "      <td>3.485117e-08</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5864</th>\n",
       "      <td>0.152854</td>\n",
       "      <td>0.693109</td>\n",
       "      <td>0.692830</td>\n",
       "      <td>0.696784</td>\n",
       "      <td>0.694129</td>\n",
       "      <td>0.656598</td>\n",
       "      <td>0.631821</td>\n",
       "      <td>0.698296</td>\n",
       "      <td>0.724781</td>\n",
       "      <td>0.722576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>4.049098e-05</td>\n",
       "      <td>8.601131e-06</td>\n",
       "      <td>3.937939e-06</td>\n",
       "      <td>2.715689e-06</td>\n",
       "      <td>3.084966e-06</td>\n",
       "      <td>2.419554e-06</td>\n",
       "      <td>3.091766e-08</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5865</th>\n",
       "      <td>0.344638</td>\n",
       "      <td>0.736154</td>\n",
       "      <td>0.749546</td>\n",
       "      <td>0.767200</td>\n",
       "      <td>0.778888</td>\n",
       "      <td>0.770688</td>\n",
       "      <td>0.709407</td>\n",
       "      <td>0.764440</td>\n",
       "      <td>0.763983</td>\n",
       "      <td>0.760305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>4.006812e-04</td>\n",
       "      <td>3.504132e-04</td>\n",
       "      <td>3.588180e-04</td>\n",
       "      <td>3.639401e-04</td>\n",
       "      <td>3.567266e-04</td>\n",
       "      <td>3.588423e-04</td>\n",
       "      <td>3.368800e-04</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5866</th>\n",
       "      <td>0.156242</td>\n",
       "      <td>0.676020</td>\n",
       "      <td>0.659184</td>\n",
       "      <td>0.661553</td>\n",
       "      <td>0.672642</td>\n",
       "      <td>0.637998</td>\n",
       "      <td>0.627909</td>\n",
       "      <td>0.673449</td>\n",
       "      <td>0.698129</td>\n",
       "      <td>0.690269</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>1.760217e-05</td>\n",
       "      <td>4.374423e-06</td>\n",
       "      <td>1.535481e-06</td>\n",
       "      <td>1.221578e-06</td>\n",
       "      <td>1.183883e-06</td>\n",
       "      <td>1.036524e-06</td>\n",
       "      <td>1.610952e-08</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5867</th>\n",
       "      <td>0.152515</td>\n",
       "      <td>0.692472</td>\n",
       "      <td>0.691252</td>\n",
       "      <td>0.695521</td>\n",
       "      <td>0.693673</td>\n",
       "      <td>0.657758</td>\n",
       "      <td>0.634265</td>\n",
       "      <td>0.698055</td>\n",
       "      <td>0.727095</td>\n",
       "      <td>0.719178</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>4.049148e-05</td>\n",
       "      <td>8.601358e-06</td>\n",
       "      <td>3.938358e-06</td>\n",
       "      <td>2.715868e-06</td>\n",
       "      <td>3.085213e-06</td>\n",
       "      <td>2.419730e-06</td>\n",
       "      <td>3.097129e-08</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5868 rows Ã— 164 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0     0.181929  0.597162  0.584452  0.663637  0.738467  0.684251  0.592274   \n",
       "1     0.373793  0.752836  0.767736  0.811567  0.837361  0.801382  0.736421   \n",
       "2     0.179078  0.581407  0.576625  0.674005  0.743081  0.647708  0.559811   \n",
       "3     0.181940  0.609392  0.576482  0.649354  0.732143  0.703664  0.598831   \n",
       "4     0.430646  0.690136  0.681142  0.712062  0.746599  0.718833  0.536348   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "5863  0.050528  0.671133  0.653239  0.666003  0.645347  0.701951  0.763060   \n",
       "5864  0.152854  0.693109  0.692830  0.696784  0.694129  0.656598  0.631821   \n",
       "5865  0.344638  0.736154  0.749546  0.767200  0.778888  0.770688  0.709407   \n",
       "5866  0.156242  0.676020  0.659184  0.661553  0.672642  0.637998  0.627909   \n",
       "5867  0.152515  0.692472  0.691252  0.695521  0.693673  0.657758  0.634265   \n",
       "\n",
       "             7         8         9  ...       154       155           156  \\\n",
       "0     0.605217  0.611734  0.713361  ...  0.001424  0.000824  2.245705e-03   \n",
       "1     0.722412  0.721140  0.749309  ...  0.002378  0.001764  3.191589e-03   \n",
       "2     0.605573  0.612860  0.715582  ...  0.000581  0.000415  8.819485e-04   \n",
       "3     0.607627  0.609744  0.704340  ...  0.001424  0.000824  2.245646e-03   \n",
       "4     0.523392  0.554430  0.522560  ...  0.006053  0.014728  2.603143e-02   \n",
       "...        ...       ...       ...  ...       ...       ...           ...   \n",
       "5863  0.787043  0.806889  0.784755  ...  0.000024  0.000003  7.980836e-07   \n",
       "5864  0.698296  0.724781  0.722576  ...  0.000081  0.000035  4.049098e-05   \n",
       "5865  0.764440  0.763983  0.760305  ...  0.000425  0.000380  4.006812e-04   \n",
       "5866  0.673449  0.698129  0.690269  ...  0.000034  0.000014  1.760217e-05   \n",
       "5867  0.698055  0.727095  0.719178  ...  0.000081  0.000035  4.049148e-05   \n",
       "\n",
       "               157           158           159           160           161  \\\n",
       "0     4.730351e-03  7.843456e-03  6.678044e-03  2.589605e-03  1.258858e-03   \n",
       "1     5.683203e-03  8.739583e-03  7.548534e-03  3.495810e-03  2.288241e-03   \n",
       "2     1.978945e-03  3.881180e-03  3.024464e-03  9.673847e-04  6.371708e-04   \n",
       "3     4.730394e-03  7.843437e-03  6.678062e-03  2.589596e-03  1.258859e-03   \n",
       "4     2.871043e-02  8.294137e-03  7.164728e-03  2.395335e-03  6.251118e-04   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "5863  9.887120e-07  8.714226e-07  7.584952e-07  8.125273e-07  4.392478e-07   \n",
       "5864  8.601131e-06  3.937939e-06  2.715689e-06  3.084966e-06  2.419554e-06   \n",
       "5865  3.504132e-04  3.588180e-04  3.639401e-04  3.567266e-04  3.588423e-04   \n",
       "5866  4.374423e-06  1.535481e-06  1.221578e-06  1.183883e-06  1.036524e-06   \n",
       "5867  8.601358e-06  3.938358e-06  2.715868e-06  3.085213e-06  2.419730e-06   \n",
       "\n",
       "               162  Emotion_Label  \n",
       "0     3.716425e-05              4  \n",
       "1     1.007357e-03              4  \n",
       "2     1.669109e-05              4  \n",
       "3     3.716428e-05              4  \n",
       "4     1.206434e-05              4  \n",
       "...            ...            ...  \n",
       "5863  3.485117e-08              7  \n",
       "5864  3.091766e-08              7  \n",
       "5865  3.368800e-04              7  \n",
       "5866  1.610952e-08              7  \n",
       "5867  3.097129e-08              7  \n",
       "\n",
       "[5868 rows x 164 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'B-Ser.csv'\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T17:06:28.956753Z",
     "iopub.status.busy": "2024-08-14T17:06:28.956389Z",
     "iopub.status.idle": "2024-08-14T17:06:28.973021Z",
     "shell.execute_reply": "2024-08-14T17:06:28.972261Z",
     "shell.execute_reply.started": "2024-08-14T17:06:28.956721Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X = df.iloc[: ,:-1].values\n",
    "Y = df['Emotion_Label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T17:06:28.974332Z",
     "iopub.status.busy": "2024-08-14T17:06:28.974066Z",
     "iopub.status.idle": "2024-08-14T17:06:28.988148Z",
     "shell.execute_reply": "2024-08-14T17:06:28.987480Z",
     "shell.execute_reply.started": "2024-08-14T17:06:28.974310Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder()\n",
    "Y = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T17:06:28.990680Z",
     "iopub.status.busy": "2024-08-14T17:06:28.990398Z",
     "iopub.status.idle": "2024-08-14T17:06:29.038880Z",
     "shell.execute_reply": "2024-08-14T17:06:29.037952Z",
     "shell.execute_reply.started": "2024-08-14T17:06:28.990658Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4694, 163), (4694, 5), (1174, 163), (1174, 5))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splitting data\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=0, train_size=0.80, shuffle=True)\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T17:06:29.040608Z",
     "iopub.status.busy": "2024-08-14T17:06:29.040193Z",
     "iopub.status.idle": "2024-08-14T17:06:29.104528Z",
     "shell.execute_reply": "2024-08-14T17:06:29.103563Z",
     "shell.execute_reply.started": "2024-08-14T17:06:29.040552Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4694, 163), (4694, 5), (1174, 163), (1174, 5))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scaling our data with sklearn's Standard scaler\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T17:06:29.106191Z",
     "iopub.status.busy": "2024-08-14T17:06:29.105849Z",
     "iopub.status.idle": "2024-08-14T17:06:29.113185Z",
     "shell.execute_reply": "2024-08-14T17:06:29.112324Z",
     "shell.execute_reply.started": "2024-08-14T17:06:29.106159Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4694, 163, 1), (4694, 5), (1174, 163, 1), (1174, 5))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# making our data compatible to model.\n",
    "x_train = np.expand_dims(x_train, axis=2)\n",
    "x_test = np.expand_dims(x_test, axis=2)\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T17:06:29.114962Z",
     "iopub.status.busy": "2024-08-14T17:06:29.114433Z",
     "iopub.status.idle": "2024-08-14T18:30:43.557977Z",
     "shell.execute_reply": "2024-08-14T18:30:43.557156Z",
     "shell.execute_reply.started": "2024-08-14T17:06:29.114931Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 163, 256)          2560      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 163, 256)         1024      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 82, 256)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 82, 256)           0         \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 82, 512)           918016    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 82, 512)          2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 41, 512)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 41, 512)           0         \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 41, 1024)          2622464   \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 41, 1024)         4096      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 21, 1024)         0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 21, 1024)          0         \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 21, 2048)          6293504   \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 21, 2048)         8192      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling1d_3 (MaxPooling  (None, 11, 2048)         0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 11, 2048)          0         \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 2048)             0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2048)              4196352   \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 2048)             8192      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1024)              2098176   \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 1024)             4096      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 5125      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,163,845\n",
      "Trainable params: 16,150,021\n",
      "Non-trainable params: 13,824\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (x_train.shape[1], 1)\n",
    "\n",
    "model = Sequential([\n",
    "    # First Conv1D Layer\n",
    "    Conv1D(256, kernel_size=9, strides=1, padding='same', activation='relu', kernel_regularizer=l2(0.0001), input_shape=input_shape),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=3, strides=2, padding='same'),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    # Second Conv1D Layer\n",
    "    Conv1D(512, kernel_size=7, strides=1, padding='same', activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=3, strides=2, padding='same'),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    # Third Conv1D Layer\n",
    "    Conv1D(1024, kernel_size=5, strides=1, padding='same', activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=3, strides=2, padding='same'),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Fourth Conv1D Layer\n",
    "    Conv1D(2048, kernel_size=3, strides=1, padding='same', activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=3, strides=2, padding='same'),\n",
    "    Dropout(0.35),\n",
    "\n",
    "    # Global Average Pooling\n",
    "    GlobalAveragePooling1D(),\n",
    "\n",
    "    # Fully connected layers\n",
    "    Dense(2048, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    Dense(1024, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    # Output layer\n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='RMSprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/audio_classification.hdf5', verbose=1, save_best_only=True)\n",
    "rlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.5, verbose=1, patience=5, min_lr=1e-7)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 2.5214 - accuracy: 0.3862\n",
      "Epoch 1: val_loss improved from inf to 5.93433, saving model to saved_models\\audio_classification.hdf5\n",
      "74/74 [==============================] - 11s 54ms/step - loss: 2.5214 - accuracy: 0.3862 - val_loss: 5.9343 - val_accuracy: 0.1627 - lr: 0.0010\n",
      "Epoch 2/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 1.9688 - accuracy: 0.4803\n",
      "Epoch 2: val_loss did not improve from 5.93433\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 1.9682 - accuracy: 0.4806 - val_loss: 6.3280 - val_accuracy: 0.1917 - lr: 0.0010\n",
      "Epoch 3/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 1.7418 - accuracy: 0.5557\n",
      "Epoch 3: val_loss did not improve from 5.93433\n",
      "74/74 [==============================] - 3s 41ms/step - loss: 1.7393 - accuracy: 0.5569 - val_loss: 10.0936 - val_accuracy: 0.1976 - lr: 0.0010\n",
      "Epoch 4/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 1.5026 - accuracy: 0.6348\n",
      "Epoch 4: val_loss improved from 5.93433 to 2.00914, saving model to saved_models\\audio_classification.hdf5\n",
      "74/74 [==============================] - 3s 44ms/step - loss: 1.5024 - accuracy: 0.6351 - val_loss: 2.0091 - val_accuracy: 0.3629 - lr: 0.0010\n",
      "Epoch 5/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 1.2802 - accuracy: 0.7217\n",
      "Epoch 5: val_loss did not improve from 2.00914\n",
      "74/74 [==============================] - 3s 40ms/step - loss: 1.2827 - accuracy: 0.7211 - val_loss: 4.9450 - val_accuracy: 0.3935 - lr: 0.0010\n",
      "Epoch 6/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 1.1276 - accuracy: 0.7746\n",
      "Epoch 6: val_loss did not improve from 2.00914\n",
      "74/74 [==============================] - 3s 40ms/step - loss: 1.1260 - accuracy: 0.7750 - val_loss: 2.3629 - val_accuracy: 0.4761 - lr: 0.0010\n",
      "Epoch 7/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.9879 - accuracy: 0.8247\n",
      "Epoch 7: val_loss did not improve from 2.00914\n",
      "74/74 [==============================] - 3s 40ms/step - loss: 0.9933 - accuracy: 0.8245 - val_loss: 4.2145 - val_accuracy: 0.3075 - lr: 0.0010\n",
      "Epoch 8/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.8816 - accuracy: 0.8540\n",
      "Epoch 8: val_loss did not improve from 2.00914\n",
      "74/74 [==============================] - 3s 40ms/step - loss: 0.8821 - accuracy: 0.8539 - val_loss: 3.1190 - val_accuracy: 0.4293 - lr: 0.0010\n",
      "Epoch 9/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.8575 - accuracy: 0.8688\n",
      "Epoch 9: val_loss improved from 2.00914 to 1.15405, saving model to saved_models\\audio_classification.hdf5\n",
      "74/74 [==============================] - 5s 64ms/step - loss: 0.8585 - accuracy: 0.8683 - val_loss: 1.1540 - val_accuracy: 0.7504 - lr: 0.0010\n",
      "Epoch 10/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.8023 - accuracy: 0.8866\n",
      "Epoch 10: val_loss did not improve from 1.15405\n",
      "74/74 [==============================] - 3s 40ms/step - loss: 0.8025 - accuracy: 0.8865 - val_loss: 1.4108 - val_accuracy: 0.6882 - lr: 0.0010\n",
      "Epoch 11/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.7468 - accuracy: 0.8998\n",
      "Epoch 11: val_loss did not improve from 1.15405\n",
      "74/74 [==============================] - 3s 40ms/step - loss: 0.7485 - accuracy: 0.8992 - val_loss: 2.0520 - val_accuracy: 0.5468 - lr: 0.0010\n",
      "Epoch 12/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.7428 - accuracy: 0.9035\n",
      "Epoch 12: val_loss improved from 1.15405 to 0.91702, saving model to saved_models\\audio_classification.hdf5\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.7424 - accuracy: 0.9037 - val_loss: 0.9170 - val_accuracy: 0.8194 - lr: 0.0010\n",
      "Epoch 13/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.7185 - accuracy: 0.9112\n",
      "Epoch 13: val_loss did not improve from 0.91702\n",
      "74/74 [==============================] - 3s 40ms/step - loss: 0.7211 - accuracy: 0.9101 - val_loss: 0.9912 - val_accuracy: 0.8220 - lr: 0.0010\n",
      "Epoch 14/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.7022 - accuracy: 0.9161\n",
      "Epoch 14: val_loss improved from 0.91702 to 0.79401, saving model to saved_models\\audio_classification.hdf5\n",
      "74/74 [==============================] - 5s 65ms/step - loss: 0.7022 - accuracy: 0.9161 - val_loss: 0.7940 - val_accuracy: 0.8671 - lr: 0.0010\n",
      "Epoch 15/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.6629 - accuracy: 0.9272\n",
      "Epoch 15: val_loss did not improve from 0.79401\n",
      "74/74 [==============================] - 3s 40ms/step - loss: 0.6637 - accuracy: 0.9267 - val_loss: 0.8929 - val_accuracy: 0.8322 - lr: 0.0010\n",
      "Epoch 16/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.6575 - accuracy: 0.9281\n",
      "Epoch 16: val_loss did not improve from 0.79401\n",
      "74/74 [==============================] - 3s 40ms/step - loss: 0.6595 - accuracy: 0.9276 - val_loss: 1.1980 - val_accuracy: 0.7802 - lr: 0.0010\n",
      "Epoch 17/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.6290 - accuracy: 0.9371\n",
      "Epoch 17: val_loss did not improve from 0.79401\n",
      "74/74 [==============================] - 3s 40ms/step - loss: 0.6290 - accuracy: 0.9372 - val_loss: 0.9121 - val_accuracy: 0.8390 - lr: 0.0010\n",
      "Epoch 18/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.6177 - accuracy: 0.9426\n",
      "Epoch 18: val_loss did not improve from 0.79401\n",
      "74/74 [==============================] - 3s 40ms/step - loss: 0.6191 - accuracy: 0.9418 - val_loss: 1.9703 - val_accuracy: 0.8501 - lr: 0.0010\n",
      "Epoch 19/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.6129 - accuracy: 0.9392\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.79401\n",
      "74/74 [==============================] - 3s 41ms/step - loss: 0.6172 - accuracy: 0.9386 - val_loss: 3.8535 - val_accuracy: 0.4267 - lr: 0.0010\n",
      "Epoch 20/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.5108 - accuracy: 0.9677\n",
      "Epoch 20: val_loss improved from 0.79401 to 0.56089, saving model to saved_models\\audio_classification.hdf5\n",
      "74/74 [==============================] - 5s 63ms/step - loss: 0.5122 - accuracy: 0.9674 - val_loss: 0.5609 - val_accuracy: 0.9480 - lr: 5.0000e-04\n",
      "Epoch 21/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.4588 - accuracy: 0.9757\n",
      "Epoch 21: val_loss improved from 0.56089 to 0.52831, saving model to saved_models\\audio_classification.hdf5\n",
      "74/74 [==============================] - 4s 46ms/step - loss: 0.4588 - accuracy: 0.9757 - val_loss: 0.5283 - val_accuracy: 0.9438 - lr: 5.0000e-04\n",
      "Epoch 22/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.4307 - accuracy: 0.9737\n",
      "Epoch 22: val_loss did not improve from 0.52831\n",
      "74/74 [==============================] - 3s 41ms/step - loss: 0.4311 - accuracy: 0.9734 - val_loss: 0.6130 - val_accuracy: 0.9165 - lr: 5.0000e-04\n",
      "Epoch 23/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.4128 - accuracy: 0.9762\n",
      "Epoch 23: val_loss did not improve from 0.52831\n",
      "74/74 [==============================] - 3s 41ms/step - loss: 0.4124 - accuracy: 0.9764 - val_loss: 0.6580 - val_accuracy: 0.8995 - lr: 5.0000e-04\n",
      "Epoch 24/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.3922 - accuracy: 0.9790\n",
      "Epoch 24: val_loss did not improve from 0.52831\n",
      "74/74 [==============================] - 3s 41ms/step - loss: 0.3947 - accuracy: 0.9787 - val_loss: 0.6348 - val_accuracy: 0.9055 - lr: 5.0000e-04\n",
      "Epoch 25/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.3876 - accuracy: 0.9797\n",
      "Epoch 25: val_loss did not improve from 0.52831\n",
      "74/74 [==============================] - 3s 41ms/step - loss: 0.3897 - accuracy: 0.9793 - val_loss: 0.9639 - val_accuracy: 0.8458 - lr: 5.0000e-04\n",
      "Epoch 26/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.3719 - accuracy: 0.9803\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.52831\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.3716 - accuracy: 0.9804 - val_loss: 0.5333 - val_accuracy: 0.9259 - lr: 5.0000e-04\n",
      "Epoch 27/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.3360 - accuracy: 0.9891\n",
      "Epoch 27: val_loss did not improve from 0.52831\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.3362 - accuracy: 0.9889 - val_loss: 0.6090 - val_accuracy: 0.9072 - lr: 2.5000e-04\n",
      "Epoch 28/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.3182 - accuracy: 0.9899\n",
      "Epoch 28: val_loss did not improve from 0.52831\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.3188 - accuracy: 0.9896 - val_loss: 0.6238 - val_accuracy: 0.9114 - lr: 2.5000e-04\n",
      "Epoch 29/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.3040 - accuracy: 0.9923\n",
      "Epoch 29: val_loss improved from 0.52831 to 0.49987, saving model to saved_models\\audio_classification.hdf5\n",
      "74/74 [==============================] - 3s 45ms/step - loss: 0.3051 - accuracy: 0.9919 - val_loss: 0.4999 - val_accuracy: 0.9404 - lr: 2.5000e-04\n",
      "Epoch 30/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.2922 - accuracy: 0.9925\n",
      "Epoch 30: val_loss did not improve from 0.49987\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.2963 - accuracy: 0.9919 - val_loss: 0.8846 - val_accuracy: 0.9267 - lr: 2.5000e-04\n",
      "Epoch 31/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.2917 - accuracy: 0.9927\n",
      "Epoch 31: val_loss improved from 0.49987 to 0.42939, saving model to saved_models\\audio_classification.hdf5\n",
      "74/74 [==============================] - 3s 47ms/step - loss: 0.2916 - accuracy: 0.9928 - val_loss: 0.4294 - val_accuracy: 0.9566 - lr: 2.5000e-04\n",
      "Epoch 32/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.2744 - accuracy: 0.9929\n",
      "Epoch 32: val_loss improved from 0.42939 to 0.42079, saving model to saved_models\\audio_classification.hdf5\n",
      "74/74 [==============================] - 4s 48ms/step - loss: 0.2743 - accuracy: 0.9930 - val_loss: 0.4208 - val_accuracy: 0.9557 - lr: 2.5000e-04\n",
      "Epoch 33/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.2766 - accuracy: 0.9917\n",
      "Epoch 33: val_loss did not improve from 0.42079\n",
      "74/74 [==============================] - 3s 45ms/step - loss: 0.2764 - accuracy: 0.9917 - val_loss: 0.5085 - val_accuracy: 0.9344 - lr: 2.5000e-04\n",
      "Epoch 34/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.2613 - accuracy: 0.9932\n",
      "Epoch 34: val_loss did not improve from 0.42079\n",
      "74/74 [==============================] - 3s 44ms/step - loss: 0.2619 - accuracy: 0.9930 - val_loss: 0.4791 - val_accuracy: 0.9472 - lr: 2.5000e-04\n",
      "Epoch 35/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.2607 - accuracy: 0.9921\n",
      "Epoch 35: val_loss improved from 0.42079 to 0.41452, saving model to saved_models\\audio_classification.hdf5\n",
      "74/74 [==============================] - 3s 47ms/step - loss: 0.2606 - accuracy: 0.9921 - val_loss: 0.4145 - val_accuracy: 0.9557 - lr: 2.5000e-04\n",
      "Epoch 36/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.2610 - accuracy: 0.9923\n",
      "Epoch 36: val_loss did not improve from 0.41452\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.2609 - accuracy: 0.9923 - val_loss: 0.4314 - val_accuracy: 0.9549 - lr: 2.5000e-04\n",
      "Epoch 37/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.2497 - accuracy: 0.9923\n",
      "Epoch 37: val_loss did not improve from 0.41452\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.2498 - accuracy: 0.9923 - val_loss: 0.4170 - val_accuracy: 0.9566 - lr: 2.5000e-04\n",
      "Epoch 38/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.2468 - accuracy: 0.9925\n",
      "Epoch 38: val_loss improved from 0.41452 to 0.40487, saving model to saved_models\\audio_classification.hdf5\n",
      "74/74 [==============================] - 3s 47ms/step - loss: 0.2483 - accuracy: 0.9919 - val_loss: 0.4049 - val_accuracy: 0.9532 - lr: 2.5000e-04\n",
      "Epoch 39/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.2461 - accuracy: 0.9925\n",
      "Epoch 39: val_loss did not improve from 0.40487\n",
      "74/74 [==============================] - 5s 71ms/step - loss: 0.2516 - accuracy: 0.9921 - val_loss: 0.5008 - val_accuracy: 0.9395 - lr: 2.5000e-04\n",
      "Epoch 40/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.2456 - accuracy: 0.9921\n",
      "Epoch 40: val_loss improved from 0.40487 to 0.38389, saving model to saved_models\\audio_classification.hdf5\n",
      "74/74 [==============================] - 3s 45ms/step - loss: 0.2471 - accuracy: 0.9919 - val_loss: 0.3839 - val_accuracy: 0.9574 - lr: 2.5000e-04\n",
      "Epoch 41/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.2321 - accuracy: 0.9921\n",
      "Epoch 41: val_loss did not improve from 0.38389\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.2320 - accuracy: 0.9921 - val_loss: 0.4727 - val_accuracy: 0.9378 - lr: 2.5000e-04\n",
      "Epoch 42/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.2302 - accuracy: 0.9923\n",
      "Epoch 42: val_loss did not improve from 0.38389\n",
      "74/74 [==============================] - 5s 70ms/step - loss: 0.2327 - accuracy: 0.9919 - val_loss: 0.7931 - val_accuracy: 0.9250 - lr: 2.5000e-04\n",
      "Epoch 43/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.2252 - accuracy: 0.9929\n",
      "Epoch 43: val_loss improved from 0.38389 to 0.38203, saving model to saved_models\\audio_classification.hdf5\n",
      "74/74 [==============================] - 3s 45ms/step - loss: 0.2289 - accuracy: 0.9923 - val_loss: 0.3820 - val_accuracy: 0.9608 - lr: 2.5000e-04\n",
      "Epoch 44/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.2275 - accuracy: 0.9925\n",
      "Epoch 44: val_loss did not improve from 0.38203\n",
      "74/74 [==============================] - 4s 50ms/step - loss: 0.2323 - accuracy: 0.9917 - val_loss: 1.3833 - val_accuracy: 0.9140 - lr: 2.5000e-04\n",
      "Epoch 45/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.2258 - accuracy: 0.9936\n",
      "Epoch 45: val_loss did not improve from 0.38203\n",
      "74/74 [==============================] - 5s 62ms/step - loss: 0.2258 - accuracy: 0.9936 - val_loss: 0.4538 - val_accuracy: 0.9480 - lr: 2.5000e-04\n",
      "Epoch 46/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.2211 - accuracy: 0.9929\n",
      "Epoch 46: val_loss improved from 0.38203 to 0.38071, saving model to saved_models\\audio_classification.hdf5\n",
      "74/74 [==============================] - 3s 45ms/step - loss: 0.2210 - accuracy: 0.9930 - val_loss: 0.3807 - val_accuracy: 0.9591 - lr: 2.5000e-04\n",
      "Epoch 47/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.2074 - accuracy: 0.9940\n",
      "Epoch 47: val_loss did not improve from 0.38071\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.2074 - accuracy: 0.9940 - val_loss: 0.4085 - val_accuracy: 0.9557 - lr: 2.5000e-04\n",
      "Epoch 48/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.2238 - accuracy: 0.9911\n",
      "Epoch 48: val_loss did not improve from 0.38071\n",
      "74/74 [==============================] - 5s 71ms/step - loss: 0.2238 - accuracy: 0.9911 - val_loss: 0.5214 - val_accuracy: 0.9395 - lr: 2.5000e-04\n",
      "Epoch 49/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.2178 - accuracy: 0.9923\n",
      "Epoch 49: val_loss did not improve from 0.38071\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.2200 - accuracy: 0.9921 - val_loss: 1.1567 - val_accuracy: 0.9089 - lr: 2.5000e-04\n",
      "Epoch 50/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.2082 - accuracy: 0.9938\n",
      "Epoch 50: val_loss did not improve from 0.38071\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.2084 - accuracy: 0.9936 - val_loss: 1.0356 - val_accuracy: 0.8305 - lr: 2.5000e-04\n",
      "Epoch 51/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.2176 - accuracy: 0.9902\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.38071\n",
      "74/74 [==============================] - 5s 69ms/step - loss: 0.2185 - accuracy: 0.9900 - val_loss: 0.6728 - val_accuracy: 0.9055 - lr: 2.5000e-04\n",
      "Epoch 52/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1983 - accuracy: 0.9944\n",
      "Epoch 52: val_loss did not improve from 0.38071\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1998 - accuracy: 0.9942 - val_loss: 0.4688 - val_accuracy: 0.9395 - lr: 1.2500e-04\n",
      "Epoch 53/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1924 - accuracy: 0.9959\n",
      "Epoch 53: val_loss did not improve from 0.38071\n",
      "74/74 [==============================] - 4s 48ms/step - loss: 0.1931 - accuracy: 0.9957 - val_loss: 0.3848 - val_accuracy: 0.9514 - lr: 1.2500e-04\n",
      "Epoch 54/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1879 - accuracy: 0.9966\n",
      "Epoch 54: val_loss did not improve from 0.38071\n",
      "74/74 [==============================] - 5s 63ms/step - loss: 0.1879 - accuracy: 0.9966 - val_loss: 0.4510 - val_accuracy: 0.9480 - lr: 1.2500e-04\n",
      "Epoch 55/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1926 - accuracy: 0.9959\n",
      "Epoch 55: val_loss did not improve from 0.38071\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1926 - accuracy: 0.9960 - val_loss: 0.3916 - val_accuracy: 0.9523 - lr: 1.2500e-04\n",
      "Epoch 56/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1794 - accuracy: 0.9970\n",
      "Epoch 56: val_loss improved from 0.38071 to 0.33490, saving model to saved_models\\audio_classification.hdf5\n",
      "74/74 [==============================] - 5s 73ms/step - loss: 0.1794 - accuracy: 0.9970 - val_loss: 0.3349 - val_accuracy: 0.9625 - lr: 1.2500e-04\n",
      "Epoch 57/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1770 - accuracy: 0.9976\n",
      "Epoch 57: val_loss improved from 0.33490 to 0.32616, saving model to saved_models\\audio_classification.hdf5\n",
      "74/74 [==============================] - 4s 47ms/step - loss: 0.1770 - accuracy: 0.9977 - val_loss: 0.3262 - val_accuracy: 0.9634 - lr: 1.2500e-04\n",
      "Epoch 58/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1730 - accuracy: 0.9970\n",
      "Epoch 58: val_loss did not improve from 0.32616\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1730 - accuracy: 0.9970 - val_loss: 0.3358 - val_accuracy: 0.9625 - lr: 1.2500e-04\n",
      "Epoch 59/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1759 - accuracy: 0.9959\n",
      "Epoch 59: val_loss improved from 0.32616 to 0.32324, saving model to saved_models\\audio_classification.hdf5\n",
      "74/74 [==============================] - 5s 73ms/step - loss: 0.1758 - accuracy: 0.9960 - val_loss: 0.3232 - val_accuracy: 0.9608 - lr: 1.2500e-04\n",
      "Epoch 60/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1700 - accuracy: 0.9968\n",
      "Epoch 60: val_loss did not improve from 0.32324\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1700 - accuracy: 0.9968 - val_loss: 0.3243 - val_accuracy: 0.9668 - lr: 1.2500e-04\n",
      "Epoch 61/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1638 - accuracy: 0.9983\n",
      "Epoch 61: val_loss did not improve from 0.32324\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1640 - accuracy: 0.9981 - val_loss: 0.3495 - val_accuracy: 0.9608 - lr: 1.2500e-04\n",
      "Epoch 62/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1667 - accuracy: 0.9970\n",
      "Epoch 62: val_loss did not improve from 0.32324\n",
      "74/74 [==============================] - 5s 66ms/step - loss: 0.1667 - accuracy: 0.9970 - val_loss: 0.3955 - val_accuracy: 0.9557 - lr: 1.2500e-04\n",
      "Epoch 63/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1645 - accuracy: 0.9957\n",
      "Epoch 63: val_loss did not improve from 0.32324\n",
      "74/74 [==============================] - 3s 46ms/step - loss: 0.1644 - accuracy: 0.9957 - val_loss: 0.3811 - val_accuracy: 0.9540 - lr: 1.2500e-04\n",
      "Epoch 64/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1683 - accuracy: 0.9944\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.32324\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1683 - accuracy: 0.9945 - val_loss: 0.3375 - val_accuracy: 0.9651 - lr: 1.2500e-04\n",
      "Epoch 65/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1646 - accuracy: 0.9964\n",
      "Epoch 65: val_loss did not improve from 0.32324\n",
      "74/74 [==============================] - 5s 72ms/step - loss: 0.1646 - accuracy: 0.9964 - val_loss: 0.3350 - val_accuracy: 0.9668 - lr: 6.2500e-05\n",
      "Epoch 66/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1558 - accuracy: 0.9979\n",
      "Epoch 66: val_loss improved from 0.32324 to 0.32053, saving model to saved_models\\audio_classification.hdf5\n",
      "74/74 [==============================] - 3s 45ms/step - loss: 0.1558 - accuracy: 0.9979 - val_loss: 0.3205 - val_accuracy: 0.9685 - lr: 6.2500e-05\n",
      "Epoch 67/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1537 - accuracy: 0.9979\n",
      "Epoch 67: val_loss did not improve from 0.32053\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1580 - accuracy: 0.9970 - val_loss: 0.3429 - val_accuracy: 0.9651 - lr: 6.2500e-05\n",
      "Epoch 68/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1504 - accuracy: 0.9991\n",
      "Epoch 68: val_loss did not improve from 0.32053\n",
      "74/74 [==============================] - 4s 59ms/step - loss: 0.1504 - accuracy: 0.9991 - val_loss: 0.3225 - val_accuracy: 0.9668 - lr: 6.2500e-05\n",
      "Epoch 69/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1511 - accuracy: 0.9981\n",
      "Epoch 69: val_loss did not improve from 0.32053\n",
      "74/74 [==============================] - 4s 52ms/step - loss: 0.1511 - accuracy: 0.9981 - val_loss: 0.3319 - val_accuracy: 0.9642 - lr: 6.2500e-05\n",
      "Epoch 70/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1521 - accuracy: 0.9972\n",
      "Epoch 70: val_loss did not improve from 0.32053\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1520 - accuracy: 0.9972 - val_loss: 0.3205 - val_accuracy: 0.9668 - lr: 6.2500e-05\n",
      "Epoch 71/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1458 - accuracy: 0.9983\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.32053\n",
      "74/74 [==============================] - 5s 68ms/step - loss: 0.1458 - accuracy: 0.9983 - val_loss: 0.3592 - val_accuracy: 0.9608 - lr: 6.2500e-05\n",
      "Epoch 72/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1482 - accuracy: 0.9977\n",
      "Epoch 72: val_loss did not improve from 0.32053\n",
      "74/74 [==============================] - 3s 44ms/step - loss: 0.1482 - accuracy: 0.9977 - val_loss: 0.3367 - val_accuracy: 0.9642 - lr: 3.1250e-05\n",
      "Epoch 73/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1437 - accuracy: 0.9991\n",
      "Epoch 73: val_loss did not improve from 0.32053\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1437 - accuracy: 0.9991 - val_loss: 0.3411 - val_accuracy: 0.9642 - lr: 3.1250e-05\n",
      "Epoch 74/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1447 - accuracy: 0.9981\n",
      "Epoch 74: val_loss did not improve from 0.32053\n",
      "74/74 [==============================] - 5s 73ms/step - loss: 0.1447 - accuracy: 0.9981 - val_loss: 0.3259 - val_accuracy: 0.9659 - lr: 3.1250e-05\n",
      "Epoch 75/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1466 - accuracy: 0.9976\n",
      "Epoch 75: val_loss improved from 0.32053 to 0.31950, saving model to saved_models\\audio_classification.hdf5\n",
      "74/74 [==============================] - 3s 44ms/step - loss: 0.1465 - accuracy: 0.9977 - val_loss: 0.3195 - val_accuracy: 0.9668 - lr: 3.1250e-05\n",
      "Epoch 76/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1415 - accuracy: 0.9983\n",
      "Epoch 76: val_loss did not improve from 0.31950\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1415 - accuracy: 0.9983 - val_loss: 0.3314 - val_accuracy: 0.9685 - lr: 3.1250e-05\n",
      "Epoch 77/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1410 - accuracy: 0.9979\n",
      "Epoch 77: val_loss improved from 0.31950 to 0.31670, saving model to saved_models\\audio_classification.hdf5\n",
      "74/74 [==============================] - 5s 74ms/step - loss: 0.1410 - accuracy: 0.9979 - val_loss: 0.3167 - val_accuracy: 0.9685 - lr: 3.1250e-05\n",
      "Epoch 78/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1433 - accuracy: 0.9985\n",
      "Epoch 78: val_loss did not improve from 0.31670\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1433 - accuracy: 0.9985 - val_loss: 0.3213 - val_accuracy: 0.9702 - lr: 3.1250e-05\n",
      "Epoch 79/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1387 - accuracy: 0.9987\n",
      "Epoch 79: val_loss did not improve from 0.31670\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1387 - accuracy: 0.9987 - val_loss: 0.3170 - val_accuracy: 0.9693 - lr: 3.1250e-05\n",
      "Epoch 80/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1384 - accuracy: 0.9985\n",
      "Epoch 80: val_loss did not improve from 0.31670\n",
      "74/74 [==============================] - 5s 70ms/step - loss: 0.1383 - accuracy: 0.9985 - val_loss: 0.3287 - val_accuracy: 0.9676 - lr: 3.1250e-05\n",
      "Epoch 81/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1379 - accuracy: 0.9985\n",
      "Epoch 81: val_loss did not improve from 0.31670\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1379 - accuracy: 0.9985 - val_loss: 0.3212 - val_accuracy: 0.9685 - lr: 3.1250e-05\n",
      "Epoch 82/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1353 - accuracy: 0.9991\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.31670\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1353 - accuracy: 0.9991 - val_loss: 0.3308 - val_accuracy: 0.9651 - lr: 3.1250e-05\n",
      "Epoch 83/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1351 - accuracy: 0.9987\n",
      "Epoch 83: val_loss did not improve from 0.31670\n",
      "74/74 [==============================] - 5s 71ms/step - loss: 0.1351 - accuracy: 0.9987 - val_loss: 0.3212 - val_accuracy: 0.9668 - lr: 1.5625e-05\n",
      "Epoch 84/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1384 - accuracy: 0.9972\n",
      "Epoch 84: val_loss improved from 0.31670 to 0.31387, saving model to saved_models\\audio_classification.hdf5\n",
      "74/74 [==============================] - 3s 45ms/step - loss: 0.1384 - accuracy: 0.9972 - val_loss: 0.3139 - val_accuracy: 0.9685 - lr: 1.5625e-05\n",
      "Epoch 85/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1355 - accuracy: 0.9991\n",
      "Epoch 85: val_loss improved from 0.31387 to 0.30901, saving model to saved_models\\audio_classification.hdf5\n",
      "74/74 [==============================] - 4s 52ms/step - loss: 0.1355 - accuracy: 0.9991 - val_loss: 0.3090 - val_accuracy: 0.9685 - lr: 1.5625e-05\n",
      "Epoch 86/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1359 - accuracy: 0.9991\n",
      "Epoch 86: val_loss improved from 0.30901 to 0.30676, saving model to saved_models\\audio_classification.hdf5\n",
      "74/74 [==============================] - 5s 67ms/step - loss: 0.1359 - accuracy: 0.9991 - val_loss: 0.3068 - val_accuracy: 0.9685 - lr: 1.5625e-05\n",
      "Epoch 87/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1326 - accuracy: 0.9991\n",
      "Epoch 87: val_loss improved from 0.30676 to 0.30512, saving model to saved_models\\audio_classification.hdf5\n",
      "74/74 [==============================] - 3s 45ms/step - loss: 0.1326 - accuracy: 0.9991 - val_loss: 0.3051 - val_accuracy: 0.9693 - lr: 1.5625e-05\n",
      "Epoch 88/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1309 - accuracy: 0.9996\n",
      "Epoch 88: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 70ms/step - loss: 0.1309 - accuracy: 0.9996 - val_loss: 0.3058 - val_accuracy: 0.9685 - lr: 1.5625e-05\n",
      "Epoch 89/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1301 - accuracy: 0.9996\n",
      "Epoch 89: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1301 - accuracy: 0.9996 - val_loss: 0.3117 - val_accuracy: 0.9676 - lr: 1.5625e-05\n",
      "Epoch 90/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1293 - accuracy: 0.9996\n",
      "Epoch 90: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1293 - accuracy: 0.9996 - val_loss: 0.3081 - val_accuracy: 0.9685 - lr: 1.5625e-05\n",
      "Epoch 91/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1311 - accuracy: 0.9989\n",
      "Epoch 91: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 71ms/step - loss: 0.1311 - accuracy: 0.9989 - val_loss: 0.3104 - val_accuracy: 0.9685 - lr: 1.5625e-05\n",
      "Epoch 92/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1289 - accuracy: 0.9989\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1288 - accuracy: 0.9989 - val_loss: 0.3101 - val_accuracy: 0.9693 - lr: 1.5625e-05\n",
      "Epoch 93/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1282 - accuracy: 0.9994\n",
      "Epoch 93: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1282 - accuracy: 0.9994 - val_loss: 0.3127 - val_accuracy: 0.9693 - lr: 7.8125e-06\n",
      "Epoch 94/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1280 - accuracy: 0.9998\n",
      "Epoch 94: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 71ms/step - loss: 0.1279 - accuracy: 0.9998 - val_loss: 0.3091 - val_accuracy: 0.9676 - lr: 7.8125e-06\n",
      "Epoch 95/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1289 - accuracy: 0.9987\n",
      "Epoch 95: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1288 - accuracy: 0.9987 - val_loss: 0.3084 - val_accuracy: 0.9685 - lr: 7.8125e-06\n",
      "Epoch 96/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1276 - accuracy: 0.9994\n",
      "Epoch 96: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1276 - accuracy: 0.9994 - val_loss: 0.3058 - val_accuracy: 0.9702 - lr: 7.8125e-06\n",
      "Epoch 97/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1272 - accuracy: 0.9991\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 70ms/step - loss: 0.1273 - accuracy: 0.9991 - val_loss: 0.3061 - val_accuracy: 0.9693 - lr: 7.8125e-06\n",
      "Epoch 98/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1273 - accuracy: 0.9994\n",
      "Epoch 98: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1273 - accuracy: 0.9994 - val_loss: 0.3077 - val_accuracy: 0.9702 - lr: 3.9063e-06\n",
      "Epoch 99/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1293 - accuracy: 0.9991\n",
      "Epoch 99: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1293 - accuracy: 0.9991 - val_loss: 0.3096 - val_accuracy: 0.9702 - lr: 3.9063e-06\n",
      "Epoch 100/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1261 - accuracy: 0.9996\n",
      "Epoch 100: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 71ms/step - loss: 0.1261 - accuracy: 0.9996 - val_loss: 0.3082 - val_accuracy: 0.9702 - lr: 3.9063e-06\n",
      "Epoch 101/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1256 - accuracy: 0.9996\n",
      "Epoch 101: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1262 - accuracy: 0.9991 - val_loss: 0.3059 - val_accuracy: 0.9702 - lr: 3.9063e-06\n",
      "Epoch 102/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1282 - accuracy: 0.9991\n",
      "Epoch 102: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 4s 52ms/step - loss: 0.1282 - accuracy: 0.9991 - val_loss: 0.3068 - val_accuracy: 0.9702 - lr: 3.9063e-06\n",
      "Epoch 103/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1271 - accuracy: 0.9989\n",
      "Epoch 103: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 60ms/step - loss: 0.1271 - accuracy: 0.9989 - val_loss: 0.3076 - val_accuracy: 0.9702 - lr: 1.9531e-06\n",
      "Epoch 104/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1253 - accuracy: 0.9996\n",
      "Epoch 104: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1253 - accuracy: 0.9996 - val_loss: 0.3069 - val_accuracy: 0.9702 - lr: 1.9531e-06\n",
      "Epoch 105/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1256 - accuracy: 0.9991\n",
      "Epoch 105: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 64ms/step - loss: 0.1256 - accuracy: 0.9991 - val_loss: 0.3066 - val_accuracy: 0.9702 - lr: 1.9531e-06\n",
      "Epoch 106/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1249 - accuracy: 1.0000\n",
      "Epoch 106: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 4s 49ms/step - loss: 0.1249 - accuracy: 1.0000 - val_loss: 0.3061 - val_accuracy: 0.9702 - lr: 1.9531e-06\n",
      "Epoch 107/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1272 - accuracy: 0.9991\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1272 - accuracy: 0.9991 - val_loss: 0.3066 - val_accuracy: 0.9702 - lr: 1.9531e-06\n",
      "Epoch 108/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1258 - accuracy: 0.9989\n",
      "Epoch 108: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 72ms/step - loss: 0.1258 - accuracy: 0.9989 - val_loss: 0.3053 - val_accuracy: 0.9693 - lr: 9.7656e-07\n",
      "Epoch 109/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1267 - accuracy: 0.9989\n",
      "Epoch 109: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1267 - accuracy: 0.9989 - val_loss: 0.3055 - val_accuracy: 0.9693 - lr: 9.7656e-07\n",
      "Epoch 110/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1265 - accuracy: 0.9991\n",
      "Epoch 110: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1265 - accuracy: 0.9991 - val_loss: 0.3064 - val_accuracy: 0.9693 - lr: 9.7656e-07\n",
      "Epoch 111/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1260 - accuracy: 0.9991\n",
      "Epoch 111: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1260 - accuracy: 0.9991 - val_loss: 0.3064 - val_accuracy: 0.9693 - lr: 9.7656e-07\n",
      "Epoch 112/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1254 - accuracy: 0.9996\n",
      "Epoch 112: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 72ms/step - loss: 0.1254 - accuracy: 0.9996 - val_loss: 0.3054 - val_accuracy: 0.9693 - lr: 9.7656e-07\n",
      "Epoch 113/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1258 - accuracy: 0.9994\n",
      "Epoch 113: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1258 - accuracy: 0.9994 - val_loss: 0.3060 - val_accuracy: 0.9693 - lr: 4.8828e-07\n",
      "Epoch 114/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1251 - accuracy: 0.9994\n",
      "Epoch 114: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 66ms/step - loss: 0.1251 - accuracy: 0.9994 - val_loss: 0.3061 - val_accuracy: 0.9693 - lr: 4.8828e-07\n",
      "Epoch 115/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1261 - accuracy: 0.9996\n",
      "Epoch 115: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 45ms/step - loss: 0.1261 - accuracy: 0.9996 - val_loss: 0.3057 - val_accuracy: 0.9693 - lr: 4.8828e-07\n",
      "Epoch 116/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1241 - accuracy: 0.9998\n",
      "Epoch 116: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1241 - accuracy: 0.9998 - val_loss: 0.3065 - val_accuracy: 0.9693 - lr: 4.8828e-07\n",
      "Epoch 117/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1265 - accuracy: 0.9987\n",
      "Epoch 117: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 72ms/step - loss: 0.1265 - accuracy: 0.9987 - val_loss: 0.3066 - val_accuracy: 0.9693 - lr: 4.8828e-07\n",
      "Epoch 118/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1276 - accuracy: 0.9987\n",
      "Epoch 118: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1276 - accuracy: 0.9987 - val_loss: 0.3064 - val_accuracy: 0.9693 - lr: 2.4414e-07\n",
      "Epoch 119/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1292 - accuracy: 0.9985\n",
      "Epoch 119: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1292 - accuracy: 0.9985 - val_loss: 0.3054 - val_accuracy: 0.9693 - lr: 2.4414e-07\n",
      "Epoch 120/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1263 - accuracy: 0.9991\n",
      "Epoch 120: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 73ms/step - loss: 0.1263 - accuracy: 0.9991 - val_loss: 0.3054 - val_accuracy: 0.9693 - lr: 2.4414e-07\n",
      "Epoch 121/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1246 - accuracy: 0.9996\n",
      "Epoch 121: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1246 - accuracy: 0.9996 - val_loss: 0.3060 - val_accuracy: 0.9693 - lr: 2.4414e-07\n",
      "Epoch 122/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1259 - accuracy: 0.9994\n",
      "Epoch 122: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1260 - accuracy: 0.9994 - val_loss: 0.3062 - val_accuracy: 0.9693 - lr: 2.4414e-07\n",
      "Epoch 123/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1248 - accuracy: 0.9998\n",
      "Epoch 123: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 70ms/step - loss: 0.1248 - accuracy: 0.9998 - val_loss: 0.3063 - val_accuracy: 0.9693 - lr: 1.2207e-07\n",
      "Epoch 124/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1262 - accuracy: 0.9994\n",
      "Epoch 124: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1262 - accuracy: 0.9994 - val_loss: 0.3060 - val_accuracy: 0.9693 - lr: 1.2207e-07\n",
      "Epoch 125/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1250 - accuracy: 0.9996\n",
      "Epoch 125: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1250 - accuracy: 0.9996 - val_loss: 0.3065 - val_accuracy: 0.9693 - lr: 1.2207e-07\n",
      "Epoch 126/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1271 - accuracy: 0.9991\n",
      "Epoch 126: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 70ms/step - loss: 0.1271 - accuracy: 0.9991 - val_loss: 0.3060 - val_accuracy: 0.9693 - lr: 1.2207e-07\n",
      "Epoch 127/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1260 - accuracy: 0.9989\n",
      "Epoch 127: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1260 - accuracy: 0.9989 - val_loss: 0.3066 - val_accuracy: 0.9693 - lr: 1.2207e-07\n",
      "Epoch 128/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1270 - accuracy: 0.9994\n",
      "Epoch 128: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1270 - accuracy: 0.9994 - val_loss: 0.3061 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 129/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1260 - accuracy: 0.9989\n",
      "Epoch 129: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 70ms/step - loss: 0.1260 - accuracy: 0.9989 - val_loss: 0.3060 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 130/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1261 - accuracy: 0.9994\n",
      "Epoch 130: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1261 - accuracy: 0.9994 - val_loss: 0.3059 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 131/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1239 - accuracy: 1.0000\n",
      "Epoch 131: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 65ms/step - loss: 0.1239 - accuracy: 1.0000 - val_loss: 0.3063 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 132/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1257 - accuracy: 0.9996\n",
      "Epoch 132: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 4s 47ms/step - loss: 0.1257 - accuracy: 0.9996 - val_loss: 0.3067 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 133/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1283 - accuracy: 0.9987\n",
      "Epoch 133: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1283 - accuracy: 0.9987 - val_loss: 0.3068 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 134/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1247 - accuracy: 0.9996\n",
      "Epoch 134: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 72ms/step - loss: 0.1247 - accuracy: 0.9996 - val_loss: 0.3067 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 135/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1276 - accuracy: 0.9985\n",
      "Epoch 135: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1276 - accuracy: 0.9985 - val_loss: 0.3063 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 136/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1267 - accuracy: 0.9989\n",
      "Epoch 136: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1267 - accuracy: 0.9989 - val_loss: 0.3066 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 137/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1255 - accuracy: 0.9991\n",
      "Epoch 137: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 71ms/step - loss: 0.1255 - accuracy: 0.9991 - val_loss: 0.3066 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 138/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1273 - accuracy: 0.9991\n",
      "Epoch 138: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1273 - accuracy: 0.9991 - val_loss: 0.3063 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 139/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1238 - accuracy: 1.0000\n",
      "Epoch 139: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1238 - accuracy: 1.0000 - val_loss: 0.3058 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 140/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1246 - accuracy: 0.9994\n",
      "Epoch 140: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 71ms/step - loss: 0.1246 - accuracy: 0.9994 - val_loss: 0.3063 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 141/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1266 - accuracy: 0.9987\n",
      "Epoch 141: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1265 - accuracy: 0.9987 - val_loss: 0.3075 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 142/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1245 - accuracy: 0.9998\n",
      "Epoch 142: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1245 - accuracy: 0.9998 - val_loss: 0.3066 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 143/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1262 - accuracy: 0.9996\n",
      "Epoch 143: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 70ms/step - loss: 0.1262 - accuracy: 0.9996 - val_loss: 0.3065 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 144/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1250 - accuracy: 0.9996\n",
      "Epoch 144: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1250 - accuracy: 0.9996 - val_loss: 0.3067 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 145/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1242 - accuracy: 1.0000\n",
      "Epoch 145: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 71ms/step - loss: 0.1242 - accuracy: 1.0000 - val_loss: 0.3065 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 146/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1253 - accuracy: 0.9996\n",
      "Epoch 146: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1253 - accuracy: 0.9996 - val_loss: 0.3062 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 147/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1262 - accuracy: 0.9994\n",
      "Epoch 147: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1263 - accuracy: 0.9994 - val_loss: 0.3063 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 148/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1266 - accuracy: 0.9987\n",
      "Epoch 148: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 73ms/step - loss: 0.1266 - accuracy: 0.9987 - val_loss: 0.3058 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 149/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1246 - accuracy: 0.9996\n",
      "Epoch 149: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1246 - accuracy: 0.9996 - val_loss: 0.3064 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 150/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1240 - accuracy: 1.0000\n",
      "Epoch 150: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1240 - accuracy: 1.0000 - val_loss: 0.3069 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 151/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1253 - accuracy: 0.9991\n",
      "Epoch 151: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 72ms/step - loss: 0.1253 - accuracy: 0.9991 - val_loss: 0.3068 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 152/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1277 - accuracy: 0.9985\n",
      "Epoch 152: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1277 - accuracy: 0.9985 - val_loss: 0.3063 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 153/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1266 - accuracy: 0.9987\n",
      "Epoch 153: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1286 - accuracy: 0.9983 - val_loss: 0.3065 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 154/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1244 - accuracy: 0.9998\n",
      "Epoch 154: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 71ms/step - loss: 0.1244 - accuracy: 0.9998 - val_loss: 0.3063 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 155/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1257 - accuracy: 0.9991\n",
      "Epoch 155: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1257 - accuracy: 0.9991 - val_loss: 0.3056 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 156/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1253 - accuracy: 0.9991\n",
      "Epoch 156: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1253 - accuracy: 0.9991 - val_loss: 0.3068 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 157/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1258 - accuracy: 0.9991\n",
      "Epoch 157: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 71ms/step - loss: 0.1258 - accuracy: 0.9991 - val_loss: 0.3070 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 158/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1237 - accuracy: 1.0000\n",
      "Epoch 158: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1237 - accuracy: 1.0000 - val_loss: 0.3068 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 159/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1256 - accuracy: 0.9994\n",
      "Epoch 159: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 4s 49ms/step - loss: 0.1258 - accuracy: 0.9991 - val_loss: 0.3068 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 160/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1246 - accuracy: 0.9998\n",
      "Epoch 160: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 64ms/step - loss: 0.1246 - accuracy: 0.9998 - val_loss: 0.3066 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 161/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1265 - accuracy: 0.9989\n",
      "Epoch 161: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1268 - accuracy: 0.9987 - val_loss: 0.3064 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 162/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1251 - accuracy: 0.9991\n",
      "Epoch 162: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 64ms/step - loss: 0.1251 - accuracy: 0.9991 - val_loss: 0.3063 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 163/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1282 - accuracy: 0.9989\n",
      "Epoch 163: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 4s 52ms/step - loss: 0.1281 - accuracy: 0.9989 - val_loss: 0.3056 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 164/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1245 - accuracy: 0.9998\n",
      "Epoch 164: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 44ms/step - loss: 0.1245 - accuracy: 0.9998 - val_loss: 0.3066 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 165/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1250 - accuracy: 0.9994\n",
      "Epoch 165: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 72ms/step - loss: 0.1250 - accuracy: 0.9994 - val_loss: 0.3063 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 166/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1246 - accuracy: 0.9998\n",
      "Epoch 166: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1246 - accuracy: 0.9998 - val_loss: 0.3068 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 167/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1256 - accuracy: 0.9991\n",
      "Epoch 167: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1256 - accuracy: 0.9991 - val_loss: 0.3066 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 168/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1274 - accuracy: 0.9991\n",
      "Epoch 168: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 70ms/step - loss: 0.1274 - accuracy: 0.9991 - val_loss: 0.3061 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 169/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1239 - accuracy: 1.0000\n",
      "Epoch 169: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1239 - accuracy: 1.0000 - val_loss: 0.3059 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 170/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1240 - accuracy: 0.9998\n",
      "Epoch 170: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1240 - accuracy: 0.9998 - val_loss: 0.3064 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 171/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1241 - accuracy: 0.9998\n",
      "Epoch 171: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 70ms/step - loss: 0.1241 - accuracy: 0.9998 - val_loss: 0.3062 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 172/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1252 - accuracy: 0.9996\n",
      "Epoch 172: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1252 - accuracy: 0.9996 - val_loss: 0.3062 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 173/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1245 - accuracy: 0.9998\n",
      "Epoch 173: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1248 - accuracy: 0.9996 - val_loss: 0.3061 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 174/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1254 - accuracy: 0.9996\n",
      "Epoch 174: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 70ms/step - loss: 0.1254 - accuracy: 0.9996 - val_loss: 0.3062 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 175/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1260 - accuracy: 0.9994\n",
      "Epoch 175: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1260 - accuracy: 0.9994 - val_loss: 0.3067 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 176/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1265 - accuracy: 0.9989\n",
      "Epoch 176: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 71ms/step - loss: 0.1265 - accuracy: 0.9989 - val_loss: 0.3068 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 177/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1249 - accuracy: 0.9996\n",
      "Epoch 177: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1249 - accuracy: 0.9996 - val_loss: 0.3075 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 178/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1243 - accuracy: 0.9998\n",
      "Epoch 178: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1243 - accuracy: 0.9998 - val_loss: 0.3074 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 179/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1267 - accuracy: 0.9991\n",
      "Epoch 179: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 70ms/step - loss: 0.1267 - accuracy: 0.9991 - val_loss: 0.3073 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 180/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1242 - accuracy: 0.9998\n",
      "Epoch 180: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1242 - accuracy: 0.9998 - val_loss: 0.3072 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 181/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1254 - accuracy: 0.9994\n",
      "Epoch 181: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1305 - accuracy: 0.9989 - val_loss: 0.3062 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 182/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1242 - accuracy: 0.9996\n",
      "Epoch 182: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 70ms/step - loss: 0.1242 - accuracy: 0.9996 - val_loss: 0.3058 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 183/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1241 - accuracy: 0.9998\n",
      "Epoch 183: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1241 - accuracy: 0.9998 - val_loss: 0.3064 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 184/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1262 - accuracy: 0.9996\n",
      "Epoch 184: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1262 - accuracy: 0.9996 - val_loss: 0.3078 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 185/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1240 - accuracy: 1.0000\n",
      "Epoch 185: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 71ms/step - loss: 0.1240 - accuracy: 1.0000 - val_loss: 0.3078 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 186/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1265 - accuracy: 0.9983\n",
      "Epoch 186: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1264 - accuracy: 0.9983 - val_loss: 0.3071 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 187/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1246 - accuracy: 0.9996\n",
      "Epoch 187: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1246 - accuracy: 0.9996 - val_loss: 0.3069 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 188/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1250 - accuracy: 0.9994\n",
      "Epoch 188: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 70ms/step - loss: 0.1250 - accuracy: 0.9994 - val_loss: 0.3072 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 189/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1251 - accuracy: 0.9994\n",
      "Epoch 189: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1251 - accuracy: 0.9994 - val_loss: 0.3072 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 190/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1242 - accuracy: 0.9996\n",
      "Epoch 190: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 46ms/step - loss: 0.1242 - accuracy: 0.9996 - val_loss: 0.3063 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 191/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1250 - accuracy: 0.9994\n",
      "Epoch 191: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 65ms/step - loss: 0.1252 - accuracy: 0.9991 - val_loss: 0.3061 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 192/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1247 - accuracy: 0.9998\n",
      "Epoch 192: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1247 - accuracy: 0.9998 - val_loss: 0.3064 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 193/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1244 - accuracy: 0.9996\n",
      "Epoch 193: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 4s 53ms/step - loss: 0.1244 - accuracy: 0.9996 - val_loss: 0.3061 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 194/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1253 - accuracy: 0.9996\n",
      "Epoch 194: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 4s 58ms/step - loss: 0.1253 - accuracy: 0.9996 - val_loss: 0.3066 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 195/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1258 - accuracy: 0.9994\n",
      "Epoch 195: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1258 - accuracy: 0.9994 - val_loss: 0.3066 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 196/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1255 - accuracy: 0.9996\n",
      "Epoch 196: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 62ms/step - loss: 0.1255 - accuracy: 0.9996 - val_loss: 0.3069 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 197/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1245 - accuracy: 0.9994\n",
      "Epoch 197: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 4s 50ms/step - loss: 0.1245 - accuracy: 0.9994 - val_loss: 0.3071 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 198/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1239 - accuracy: 0.9998\n",
      "Epoch 198: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1239 - accuracy: 0.9998 - val_loss: 0.3072 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 199/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1239 - accuracy: 0.9998\n",
      "Epoch 199: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 70ms/step - loss: 0.1239 - accuracy: 0.9998 - val_loss: 0.3073 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 200/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1281 - accuracy: 0.9987\n",
      "Epoch 200: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1281 - accuracy: 0.9987 - val_loss: 0.3067 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 201/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1258 - accuracy: 0.9991\n",
      "Epoch 201: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 44ms/step - loss: 0.1258 - accuracy: 0.9991 - val_loss: 0.3065 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 202/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1251 - accuracy: 0.9994\n",
      "Epoch 202: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 67ms/step - loss: 0.1251 - accuracy: 0.9994 - val_loss: 0.3068 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 203/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1259 - accuracy: 0.9996\n",
      "Epoch 203: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1259 - accuracy: 0.9996 - val_loss: 0.3066 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 204/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1250 - accuracy: 0.9994\n",
      "Epoch 204: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 71ms/step - loss: 0.1250 - accuracy: 0.9994 - val_loss: 0.3065 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 205/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1247 - accuracy: 0.9991\n",
      "Epoch 205: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1268 - accuracy: 0.9989 - val_loss: 0.3061 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 206/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1247 - accuracy: 0.9994\n",
      "Epoch 206: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1247 - accuracy: 0.9994 - val_loss: 0.3069 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 207/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1244 - accuracy: 0.9996\n",
      "Epoch 207: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 71ms/step - loss: 0.1244 - accuracy: 0.9996 - val_loss: 0.3066 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 208/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1237 - accuracy: 1.0000\n",
      "Epoch 208: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1237 - accuracy: 1.0000 - val_loss: 0.3065 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 209/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1263 - accuracy: 0.9989\n",
      "Epoch 209: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 67ms/step - loss: 0.1263 - accuracy: 0.9989 - val_loss: 0.3070 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 210/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1238 - accuracy: 0.9998\n",
      "Epoch 210: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 45ms/step - loss: 0.1238 - accuracy: 0.9998 - val_loss: 0.3067 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 211/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1247 - accuracy: 0.9994\n",
      "Epoch 211: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1247 - accuracy: 0.9994 - val_loss: 0.3064 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 212/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1251 - accuracy: 0.9994\n",
      "Epoch 212: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 71ms/step - loss: 0.1251 - accuracy: 0.9994 - val_loss: 0.3068 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 213/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1247 - accuracy: 0.9994\n",
      "Epoch 213: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1247 - accuracy: 0.9994 - val_loss: 0.3070 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 214/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1237 - accuracy: 1.0000\n",
      "Epoch 214: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1237 - accuracy: 1.0000 - val_loss: 0.3059 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 215/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1257 - accuracy: 0.9994\n",
      "Epoch 215: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 71ms/step - loss: 0.1257 - accuracy: 0.9994 - val_loss: 0.3070 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 216/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1252 - accuracy: 0.9989\n",
      "Epoch 216: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1252 - accuracy: 0.9989 - val_loss: 0.3065 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 217/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1255 - accuracy: 0.9994\n",
      "Epoch 217: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1255 - accuracy: 0.9994 - val_loss: 0.3064 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 218/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1234 - accuracy: 1.0000\n",
      "Epoch 218: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 70ms/step - loss: 0.1242 - accuracy: 0.9996 - val_loss: 0.3063 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 219/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1245 - accuracy: 0.9994\n",
      "Epoch 219: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1245 - accuracy: 0.9994 - val_loss: 0.3061 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 220/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1255 - accuracy: 0.9989\n",
      "Epoch 220: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1255 - accuracy: 0.9989 - val_loss: 0.3073 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 221/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1258 - accuracy: 0.9991\n",
      "Epoch 221: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 70ms/step - loss: 0.1258 - accuracy: 0.9991 - val_loss: 0.3067 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 222/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1276 - accuracy: 0.9983\n",
      "Epoch 222: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1277 - accuracy: 0.9983 - val_loss: 0.3072 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 223/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1266 - accuracy: 0.9987\n",
      "Epoch 223: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1270 - accuracy: 0.9985 - val_loss: 0.3065 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 224/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1253 - accuracy: 0.9991\n",
      "Epoch 224: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 70ms/step - loss: 0.1255 - accuracy: 0.9989 - val_loss: 0.3072 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 225/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1259 - accuracy: 0.9987\n",
      "Epoch 225: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1259 - accuracy: 0.9987 - val_loss: 0.3061 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 226/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1251 - accuracy: 0.9994\n",
      "Epoch 226: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1251 - accuracy: 0.9994 - val_loss: 0.3058 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 227/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1257 - accuracy: 0.9996\n",
      "Epoch 227: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 70ms/step - loss: 0.1257 - accuracy: 0.9996 - val_loss: 0.3064 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 228/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1260 - accuracy: 0.9991\n",
      "Epoch 228: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1261 - accuracy: 0.9991 - val_loss: 0.3066 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 229/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1274 - accuracy: 0.9985\n",
      "Epoch 229: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1274 - accuracy: 0.9985 - val_loss: 0.3072 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 230/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1267 - accuracy: 0.9994\n",
      "Epoch 230: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 70ms/step - loss: 0.1267 - accuracy: 0.9994 - val_loss: 0.3064 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 231/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1265 - accuracy: 0.9989\n",
      "Epoch 231: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1267 - accuracy: 0.9987 - val_loss: 0.3064 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 232/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1237 - accuracy: 0.9998\n",
      "Epoch 232: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1237 - accuracy: 0.9998 - val_loss: 0.3071 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 233/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1241 - accuracy: 0.9998\n",
      "Epoch 233: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 71ms/step - loss: 0.1241 - accuracy: 0.9998 - val_loss: 0.3068 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 234/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1254 - accuracy: 0.9989\n",
      "Epoch 234: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1254 - accuracy: 0.9989 - val_loss: 0.3068 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 235/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1258 - accuracy: 0.9991\n",
      "Epoch 235: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 71ms/step - loss: 0.1258 - accuracy: 0.9991 - val_loss: 0.3062 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 236/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1280 - accuracy: 0.9989\n",
      "Epoch 236: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1286 - accuracy: 0.9987 - val_loss: 0.3062 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 237/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1246 - accuracy: 0.9994\n",
      "Epoch 237: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1246 - accuracy: 0.9994 - val_loss: 0.3082 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 238/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1237 - accuracy: 0.9998\n",
      "Epoch 238: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 70ms/step - loss: 0.1237 - accuracy: 0.9998 - val_loss: 0.3074 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 239/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1241 - accuracy: 0.9996\n",
      "Epoch 239: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1241 - accuracy: 0.9996 - val_loss: 0.3065 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 240/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1236 - accuracy: 0.9998\n",
      "Epoch 240: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 4s 52ms/step - loss: 0.1236 - accuracy: 0.9998 - val_loss: 0.3068 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 241/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1263 - accuracy: 0.9989\n",
      "Epoch 241: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 4s 60ms/step - loss: 0.1263 - accuracy: 0.9989 - val_loss: 0.3073 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 242/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1236 - accuracy: 1.0000\n",
      "Epoch 242: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1236 - accuracy: 1.0000 - val_loss: 0.3065 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 243/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1251 - accuracy: 0.9994\n",
      "Epoch 243: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 70ms/step - loss: 0.1251 - accuracy: 0.9994 - val_loss: 0.3066 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 244/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1255 - accuracy: 0.9989\n",
      "Epoch 244: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1255 - accuracy: 0.9989 - val_loss: 0.3064 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 245/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1237 - accuracy: 1.0000\n",
      "Epoch 245: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1237 - accuracy: 1.0000 - val_loss: 0.3057 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 246/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1250 - accuracy: 0.9991\n",
      "Epoch 246: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 70ms/step - loss: 0.1250 - accuracy: 0.9991 - val_loss: 0.3068 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 247/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1257 - accuracy: 0.9994\n",
      "Epoch 247: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1258 - accuracy: 0.9994 - val_loss: 0.3066 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 248/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1261 - accuracy: 0.9991\n",
      "Epoch 248: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1261 - accuracy: 0.9991 - val_loss: 0.3064 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 249/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1252 - accuracy: 0.9989\n",
      "Epoch 249: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 70ms/step - loss: 0.1252 - accuracy: 0.9989 - val_loss: 0.3072 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 250/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1241 - accuracy: 0.9994\n",
      "Epoch 250: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1241 - accuracy: 0.9994 - val_loss: 0.3064 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 251/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1254 - accuracy: 0.9989\n",
      "Epoch 251: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1254 - accuracy: 0.9989 - val_loss: 0.3067 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 252/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1236 - accuracy: 0.9998\n",
      "Epoch 252: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 70ms/step - loss: 0.1236 - accuracy: 0.9998 - val_loss: 0.3076 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 253/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1270 - accuracy: 0.9994\n",
      "Epoch 253: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1269 - accuracy: 0.9994 - val_loss: 0.3077 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 254/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1249 - accuracy: 0.9994\n",
      "Epoch 254: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1249 - accuracy: 0.9994 - val_loss: 0.3065 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 255/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1256 - accuracy: 0.9994\n",
      "Epoch 255: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 71ms/step - loss: 0.1256 - accuracy: 0.9994 - val_loss: 0.3070 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 256/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1267 - accuracy: 0.9989\n",
      "Epoch 256: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1267 - accuracy: 0.9989 - val_loss: 0.3068 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 257/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1246 - accuracy: 0.9996\n",
      "Epoch 257: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 63ms/step - loss: 0.1246 - accuracy: 0.9996 - val_loss: 0.3066 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 258/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1247 - accuracy: 0.9994\n",
      "Epoch 258: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 4s 49ms/step - loss: 0.1257 - accuracy: 0.9987 - val_loss: 0.3070 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 259/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1250 - accuracy: 0.9994\n",
      "Epoch 259: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1251 - accuracy: 0.9994 - val_loss: 0.3072 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 260/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1238 - accuracy: 0.9998\n",
      "Epoch 260: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 71ms/step - loss: 0.1238 - accuracy: 0.9998 - val_loss: 0.3071 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 261/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1245 - accuracy: 0.9996\n",
      "Epoch 261: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1258 - accuracy: 0.9994 - val_loss: 0.3082 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 262/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1256 - accuracy: 0.9991\n",
      "Epoch 262: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1256 - accuracy: 0.9991 - val_loss: 0.3077 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 263/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1249 - accuracy: 0.9994\n",
      "Epoch 263: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 71ms/step - loss: 0.1249 - accuracy: 0.9994 - val_loss: 0.3071 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 264/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1262 - accuracy: 0.9991\n",
      "Epoch 264: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1266 - accuracy: 0.9989 - val_loss: 0.3068 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 265/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1250 - accuracy: 0.9989\n",
      "Epoch 265: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 4s 54ms/step - loss: 0.1250 - accuracy: 0.9989 - val_loss: 0.3065 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 266/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1246 - accuracy: 0.9994\n",
      "Epoch 266: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 4s 58ms/step - loss: 0.1246 - accuracy: 0.9994 - val_loss: 0.3064 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 267/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1259 - accuracy: 0.9996\n",
      "Epoch 267: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1259 - accuracy: 0.9996 - val_loss: 0.3065 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 268/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1257 - accuracy: 0.9996\n",
      "Epoch 268: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 65ms/step - loss: 0.1257 - accuracy: 0.9996 - val_loss: 0.3075 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 269/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1274 - accuracy: 0.9991\n",
      "Epoch 269: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 4s 47ms/step - loss: 0.1274 - accuracy: 0.9991 - val_loss: 0.3077 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 270/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1249 - accuracy: 0.9994\n",
      "Epoch 270: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1255 - accuracy: 0.9989 - val_loss: 0.3079 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 271/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1274 - accuracy: 0.9985\n",
      "Epoch 271: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 70ms/step - loss: 0.1274 - accuracy: 0.9985 - val_loss: 0.3072 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 272/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1239 - accuracy: 0.9996\n",
      "Epoch 272: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1252 - accuracy: 0.9994 - val_loss: 0.3074 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 273/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1236 - accuracy: 1.0000\n",
      "Epoch 273: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 46ms/step - loss: 0.1236 - accuracy: 1.0000 - val_loss: 0.3073 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 274/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1266 - accuracy: 0.9985\n",
      "Epoch 274: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 65ms/step - loss: 0.1266 - accuracy: 0.9985 - val_loss: 0.3081 - val_accuracy: 0.9685 - lr: 1.0000e-07\n",
      "Epoch 275/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1237 - accuracy: 0.9998\n",
      "Epoch 275: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1238 - accuracy: 0.9998 - val_loss: 0.3078 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 276/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1237 - accuracy: 0.9998\n",
      "Epoch 276: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 44ms/step - loss: 0.1237 - accuracy: 0.9998 - val_loss: 0.3062 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 277/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1252 - accuracy: 0.9996\n",
      "Epoch 277: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 62ms/step - loss: 0.1252 - accuracy: 0.9996 - val_loss: 0.3063 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 278/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1234 - accuracy: 1.0000\n",
      "Epoch 278: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 4s 50ms/step - loss: 0.1234 - accuracy: 1.0000 - val_loss: 0.3063 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 279/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1255 - accuracy: 0.9994\n",
      "Epoch 279: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1255 - accuracy: 0.9994 - val_loss: 0.3070 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 280/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1249 - accuracy: 0.9996\n",
      "Epoch 280: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 73ms/step - loss: 0.1249 - accuracy: 0.9996 - val_loss: 0.3072 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 281/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1240 - accuracy: 0.9996\n",
      "Epoch 281: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1240 - accuracy: 0.9996 - val_loss: 0.3077 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 282/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1258 - accuracy: 0.9994\n",
      "Epoch 282: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 44ms/step - loss: 0.1258 - accuracy: 0.9994 - val_loss: 0.3079 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 283/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1238 - accuracy: 0.9994\n",
      "Epoch 283: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 72ms/step - loss: 0.1238 - accuracy: 0.9994 - val_loss: 0.3069 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 284/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1235 - accuracy: 1.0000\n",
      "Epoch 284: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 44ms/step - loss: 0.1235 - accuracy: 1.0000 - val_loss: 0.3064 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 285/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1242 - accuracy: 0.9996\n",
      "Epoch 285: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 44ms/step - loss: 0.1242 - accuracy: 0.9996 - val_loss: 0.3058 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 286/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1253 - accuracy: 0.9987\n",
      "Epoch 286: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 6s 76ms/step - loss: 0.1253 - accuracy: 0.9987 - val_loss: 0.3070 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 287/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1252 - accuracy: 0.9991\n",
      "Epoch 287: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1252 - accuracy: 0.9991 - val_loss: 0.3067 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 288/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1258 - accuracy: 0.9994\n",
      "Epoch 288: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1258 - accuracy: 0.9994 - val_loss: 0.3069 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 289/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1237 - accuracy: 0.9998\n",
      "Epoch 289: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 44ms/step - loss: 0.1237 - accuracy: 0.9998 - val_loss: 0.3065 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 290/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1246 - accuracy: 0.9994\n",
      "Epoch 290: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 73ms/step - loss: 0.1246 - accuracy: 0.9994 - val_loss: 0.3064 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 291/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1242 - accuracy: 0.9998\n",
      "Epoch 291: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1261 - accuracy: 0.9991 - val_loss: 0.3075 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 292/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1245 - accuracy: 0.9996\n",
      "Epoch 292: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 4s 56ms/step - loss: 0.1245 - accuracy: 0.9996 - val_loss: 0.3068 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 293/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1256 - accuracy: 0.9989\n",
      "Epoch 293: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 4s 58ms/step - loss: 0.1256 - accuracy: 0.9989 - val_loss: 0.3064 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 294/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1235 - accuracy: 0.9998\n",
      "Epoch 294: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1235 - accuracy: 0.9998 - val_loss: 0.3066 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 295/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1253 - accuracy: 0.9991\n",
      "Epoch 295: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 74ms/step - loss: 0.1253 - accuracy: 0.9991 - val_loss: 0.3064 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 296/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1246 - accuracy: 0.9994\n",
      "Epoch 296: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1246 - accuracy: 0.9994 - val_loss: 0.3065 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 297/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1247 - accuracy: 0.9996\n",
      "Epoch 297: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1247 - accuracy: 0.9996 - val_loss: 0.3064 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 298/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1239 - accuracy: 0.9996\n",
      "Epoch 298: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 62ms/step - loss: 0.1239 - accuracy: 0.9996 - val_loss: 0.3065 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 299/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1255 - accuracy: 0.9991\n",
      "Epoch 299: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 4s 53ms/step - loss: 0.1255 - accuracy: 0.9991 - val_loss: 0.3067 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 300/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1261 - accuracy: 0.9991\n",
      "Epoch 300: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1267 - accuracy: 0.9989 - val_loss: 0.3075 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 301/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1238 - accuracy: 0.9996\n",
      "Epoch 301: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 73ms/step - loss: 0.1238 - accuracy: 0.9996 - val_loss: 0.3074 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 302/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1248 - accuracy: 0.9991\n",
      "Epoch 302: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1248 - accuracy: 0.9991 - val_loss: 0.3067 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 303/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1254 - accuracy: 0.9987\n",
      "Epoch 303: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1254 - accuracy: 0.9987 - val_loss: 0.3062 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 304/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1234 - accuracy: 0.9998\n",
      "Epoch 304: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 72ms/step - loss: 0.1234 - accuracy: 0.9998 - val_loss: 0.3067 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 305/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1242 - accuracy: 0.9996\n",
      "Epoch 305: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1253 - accuracy: 0.9994 - val_loss: 0.3071 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 306/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1255 - accuracy: 0.9987\n",
      "Epoch 306: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1366 - accuracy: 0.9974 - val_loss: 0.3070 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 307/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1261 - accuracy: 0.9991\n",
      "Epoch 307: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 73ms/step - loss: 0.1270 - accuracy: 0.9989 - val_loss: 0.3072 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 308/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1232 - accuracy: 1.0000\n",
      "Epoch 308: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1232 - accuracy: 1.0000 - val_loss: 0.3075 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 309/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1239 - accuracy: 0.9996\n",
      "Epoch 309: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1239 - accuracy: 0.9996 - val_loss: 0.3072 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 310/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1243 - accuracy: 0.9998\n",
      "Epoch 310: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 72ms/step - loss: 0.1242 - accuracy: 0.9998 - val_loss: 0.3076 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 311/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1247 - accuracy: 0.9996\n",
      "Epoch 311: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1247 - accuracy: 0.9996 - val_loss: 0.3070 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 312/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1251 - accuracy: 0.9994\n",
      "Epoch 312: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 71ms/step - loss: 0.1251 - accuracy: 0.9994 - val_loss: 0.3076 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 313/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1257 - accuracy: 0.9998\n",
      "Epoch 313: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1257 - accuracy: 0.9998 - val_loss: 0.3065 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 314/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1235 - accuracy: 0.9996\n",
      "Epoch 314: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1235 - accuracy: 0.9996 - val_loss: 0.3074 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 315/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1234 - accuracy: 0.9998\n",
      "Epoch 315: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 72ms/step - loss: 0.1234 - accuracy: 0.9998 - val_loss: 0.3070 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 316/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1246 - accuracy: 0.9994\n",
      "Epoch 316: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1247 - accuracy: 0.9994 - val_loss: 0.3064 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 317/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1241 - accuracy: 0.9991\n",
      "Epoch 317: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 62ms/step - loss: 0.1241 - accuracy: 0.9991 - val_loss: 0.3057 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 318/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1260 - accuracy: 0.9991\n",
      "Epoch 318: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 4s 51ms/step - loss: 0.1260 - accuracy: 0.9991 - val_loss: 0.3071 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 319/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1234 - accuracy: 0.9998\n",
      "Epoch 319: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1234 - accuracy: 0.9998 - val_loss: 0.3066 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 320/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1241 - accuracy: 0.9996\n",
      "Epoch 320: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 73ms/step - loss: 0.1241 - accuracy: 0.9996 - val_loss: 0.3062 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 321/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1245 - accuracy: 0.9991\n",
      "Epoch 321: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1245 - accuracy: 0.9991 - val_loss: 0.3072 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 322/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1247 - accuracy: 0.9996\n",
      "Epoch 322: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1247 - accuracy: 0.9996 - val_loss: 0.3071 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 323/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1255 - accuracy: 0.9991\n",
      "Epoch 323: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 72ms/step - loss: 0.1255 - accuracy: 0.9991 - val_loss: 0.3070 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 324/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1237 - accuracy: 0.9996\n",
      "Epoch 324: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1240 - accuracy: 0.9994 - val_loss: 0.3073 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 325/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1277 - accuracy: 0.9991\n",
      "Epoch 325: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1277 - accuracy: 0.9991 - val_loss: 0.3065 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 326/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1232 - accuracy: 1.0000\n",
      "Epoch 326: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 72ms/step - loss: 0.1232 - accuracy: 1.0000 - val_loss: 0.3063 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 327/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1234 - accuracy: 0.9996\n",
      "Epoch 327: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1234 - accuracy: 0.9996 - val_loss: 0.3065 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 328/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1240 - accuracy: 0.9996\n",
      "Epoch 328: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1240 - accuracy: 0.9996 - val_loss: 0.3063 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 329/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1233 - accuracy: 0.9998\n",
      "Epoch 329: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 71ms/step - loss: 0.1233 - accuracy: 0.9998 - val_loss: 0.3064 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 330/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1244 - accuracy: 0.9994\n",
      "Epoch 330: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1243 - accuracy: 0.9994 - val_loss: 0.3064 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 331/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1275 - accuracy: 0.9985\n",
      "Epoch 331: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1274 - accuracy: 0.9985 - val_loss: 0.3069 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 332/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1241 - accuracy: 0.9994\n",
      "Epoch 332: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 71ms/step - loss: 0.1241 - accuracy: 0.9994 - val_loss: 0.3069 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 333/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1238 - accuracy: 0.9998\n",
      "Epoch 333: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1238 - accuracy: 0.9998 - val_loss: 0.3064 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 334/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1242 - accuracy: 0.9994\n",
      "Epoch 334: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 73ms/step - loss: 0.1242 - accuracy: 0.9994 - val_loss: 0.3069 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 335/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1244 - accuracy: 0.9991\n",
      "Epoch 335: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1244 - accuracy: 0.9991 - val_loss: 0.3071 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 336/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1248 - accuracy: 0.9989\n",
      "Epoch 336: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1248 - accuracy: 0.9989 - val_loss: 0.3067 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 337/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1236 - accuracy: 0.9996\n",
      "Epoch 337: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 73ms/step - loss: 0.1236 - accuracy: 0.9996 - val_loss: 0.3065 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 338/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1257 - accuracy: 0.9987\n",
      "Epoch 338: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1257 - accuracy: 0.9987 - val_loss: 0.3070 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 339/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1250 - accuracy: 0.9987\n",
      "Epoch 339: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1250 - accuracy: 0.9987 - val_loss: 0.3065 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 340/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1246 - accuracy: 0.9996\n",
      "Epoch 340: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 72ms/step - loss: 0.1246 - accuracy: 0.9996 - val_loss: 0.3065 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 341/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1270 - accuracy: 0.9989\n",
      "Epoch 341: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1270 - accuracy: 0.9989 - val_loss: 0.3066 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 342/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1254 - accuracy: 0.9994\n",
      "Epoch 342: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1254 - accuracy: 0.9994 - val_loss: 0.3071 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 343/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1236 - accuracy: 0.9994\n",
      "Epoch 343: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 72ms/step - loss: 0.1247 - accuracy: 0.9991 - val_loss: 0.3071 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 344/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1254 - accuracy: 0.9991\n",
      "Epoch 344: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1254 - accuracy: 0.9991 - val_loss: 0.3072 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 345/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1252 - accuracy: 0.9987\n",
      "Epoch 345: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 72ms/step - loss: 0.1252 - accuracy: 0.9987 - val_loss: 0.3067 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 346/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1245 - accuracy: 0.9991\n",
      "Epoch 346: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1245 - accuracy: 0.9991 - val_loss: 0.3072 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 347/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1261 - accuracy: 0.9989\n",
      "Epoch 347: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1261 - accuracy: 0.9989 - val_loss: 0.3069 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 348/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1237 - accuracy: 0.9996\n",
      "Epoch 348: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 71ms/step - loss: 0.1237 - accuracy: 0.9996 - val_loss: 0.3073 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 349/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1241 - accuracy: 0.9994\n",
      "Epoch 349: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1242 - accuracy: 0.9994 - val_loss: 0.3068 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 350/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1246 - accuracy: 0.9994\n",
      "Epoch 350: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1246 - accuracy: 0.9994 - val_loss: 0.3076 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 351/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1285 - accuracy: 0.9985\n",
      "Epoch 351: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 71ms/step - loss: 0.1285 - accuracy: 0.9985 - val_loss: 0.3074 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 352/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1262 - accuracy: 0.9991\n",
      "Epoch 352: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1276 - accuracy: 0.9987 - val_loss: 0.3078 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 353/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1250 - accuracy: 0.9989\n",
      "Epoch 353: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 72ms/step - loss: 0.1250 - accuracy: 0.9989 - val_loss: 0.3083 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 354/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1235 - accuracy: 0.9996\n",
      "Epoch 354: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 41ms/step - loss: 0.1235 - accuracy: 0.9996 - val_loss: 0.3068 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 355/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1250 - accuracy: 0.9987\n",
      "Epoch 355: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1250 - accuracy: 0.9987 - val_loss: 0.3069 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 356/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1241 - accuracy: 0.9996\n",
      "Epoch 356: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 71ms/step - loss: 0.1241 - accuracy: 0.9996 - val_loss: 0.3072 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 357/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1267 - accuracy: 0.9983\n",
      "Epoch 357: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1266 - accuracy: 0.9983 - val_loss: 0.3072 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 358/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1246 - accuracy: 0.9996\n",
      "Epoch 358: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 4s 58ms/step - loss: 0.1246 - accuracy: 0.9996 - val_loss: 0.3070 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 359/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1261 - accuracy: 0.9994\n",
      "Epoch 359: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 4s 54ms/step - loss: 0.1261 - accuracy: 0.9994 - val_loss: 0.3069 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 360/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1241 - accuracy: 0.9996\n",
      "Epoch 360: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1241 - accuracy: 0.9996 - val_loss: 0.3067 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 361/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1240 - accuracy: 0.9998\n",
      "Epoch 361: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 66ms/step - loss: 0.1240 - accuracy: 0.9998 - val_loss: 0.3068 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 362/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1242 - accuracy: 0.9994\n",
      "Epoch 362: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 4s 47ms/step - loss: 0.1246 - accuracy: 0.9991 - val_loss: 0.3064 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 363/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1240 - accuracy: 0.9994\n",
      "Epoch 363: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1240 - accuracy: 0.9994 - val_loss: 0.3073 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 364/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1286 - accuracy: 0.9985\n",
      "Epoch 364: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 73ms/step - loss: 0.1286 - accuracy: 0.9985 - val_loss: 0.3070 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 365/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1235 - accuracy: 0.9996\n",
      "Epoch 365: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1235 - accuracy: 0.9996 - val_loss: 0.3071 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 366/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1237 - accuracy: 0.9994\n",
      "Epoch 366: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1257 - accuracy: 0.9989 - val_loss: 0.3075 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 367/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1229 - accuracy: 0.9998\n",
      "Epoch 367: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 72ms/step - loss: 0.1229 - accuracy: 0.9998 - val_loss: 0.3070 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 368/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1233 - accuracy: 0.9996\n",
      "Epoch 368: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1233 - accuracy: 0.9996 - val_loss: 0.3075 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 369/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1244 - accuracy: 0.9994\n",
      "Epoch 369: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1244 - accuracy: 0.9994 - val_loss: 0.3067 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 370/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1244 - accuracy: 0.9994\n",
      "Epoch 370: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 71ms/step - loss: 0.1244 - accuracy: 0.9994 - val_loss: 0.3068 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 371/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1235 - accuracy: 0.9996\n",
      "Epoch 371: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1235 - accuracy: 0.9996 - val_loss: 0.3072 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 372/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1242 - accuracy: 0.9996\n",
      "Epoch 372: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1241 - accuracy: 0.9996 - val_loss: 0.3072 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 373/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1228 - accuracy: 1.0000\n",
      "Epoch 373: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 71ms/step - loss: 0.1228 - accuracy: 1.0000 - val_loss: 0.3070 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 374/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1241 - accuracy: 0.9994\n",
      "Epoch 374: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1241 - accuracy: 0.9994 - val_loss: 0.3066 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 375/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1251 - accuracy: 0.9994\n",
      "Epoch 375: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1252 - accuracy: 0.9994 - val_loss: 0.3075 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 376/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1250 - accuracy: 0.9989\n",
      "Epoch 376: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 72ms/step - loss: 0.1250 - accuracy: 0.9989 - val_loss: 0.3072 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 377/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1247 - accuracy: 0.9994\n",
      "Epoch 377: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1250 - accuracy: 0.9991 - val_loss: 0.3072 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 378/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1232 - accuracy: 0.9998\n",
      "Epoch 378: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 4s 58ms/step - loss: 0.1232 - accuracy: 0.9998 - val_loss: 0.3067 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 379/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1240 - accuracy: 0.9998\n",
      "Epoch 379: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 4s 55ms/step - loss: 0.1240 - accuracy: 0.9998 - val_loss: 0.3075 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 380/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1246 - accuracy: 0.9991\n",
      "Epoch 380: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1272 - accuracy: 0.9987 - val_loss: 0.3071 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 381/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1243 - accuracy: 0.9994\n",
      "Epoch 381: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 73ms/step - loss: 0.1243 - accuracy: 0.9994 - val_loss: 0.3067 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 382/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1232 - accuracy: 0.9998\n",
      "Epoch 382: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1232 - accuracy: 0.9998 - val_loss: 0.3066 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 383/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1239 - accuracy: 0.9991\n",
      "Epoch 383: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1240 - accuracy: 0.9991 - val_loss: 0.3069 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 384/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1245 - accuracy: 0.9996\n",
      "Epoch 384: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 73ms/step - loss: 0.1245 - accuracy: 0.9996 - val_loss: 0.3073 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 385/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1249 - accuracy: 0.9994\n",
      "Epoch 385: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 41ms/step - loss: 0.1249 - accuracy: 0.9994 - val_loss: 0.3075 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 386/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1241 - accuracy: 0.9994\n",
      "Epoch 386: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1241 - accuracy: 0.9994 - val_loss: 0.3068 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 387/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1242 - accuracy: 0.9996\n",
      "Epoch 387: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 73ms/step - loss: 0.1242 - accuracy: 0.9996 - val_loss: 0.3065 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 388/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1251 - accuracy: 0.9989\n",
      "Epoch 388: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1251 - accuracy: 0.9989 - val_loss: 0.3074 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 389/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1246 - accuracy: 0.9991\n",
      "Epoch 389: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1246 - accuracy: 0.9991 - val_loss: 0.3066 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 390/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1257 - accuracy: 0.9994\n",
      "Epoch 390: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 72ms/step - loss: 0.1257 - accuracy: 0.9994 - val_loss: 0.3066 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 391/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1242 - accuracy: 0.9991\n",
      "Epoch 391: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1242 - accuracy: 0.9991 - val_loss: 0.3066 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 392/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1246 - accuracy: 0.9994\n",
      "Epoch 392: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 72ms/step - loss: 0.1263 - accuracy: 0.9991 - val_loss: 0.3074 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 393/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1279 - accuracy: 0.9991\n",
      "Epoch 393: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1293 - accuracy: 0.9989 - val_loss: 0.3074 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 394/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1255 - accuracy: 0.9989\n",
      "Epoch 394: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1255 - accuracy: 0.9989 - val_loss: 0.3077 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 395/400\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1259 - accuracy: 0.9991\n",
      "Epoch 395: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 71ms/step - loss: 0.1259 - accuracy: 0.9991 - val_loss: 0.3067 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 396/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1258 - accuracy: 0.9991\n",
      "Epoch 396: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1258 - accuracy: 0.9991 - val_loss: 0.3058 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 397/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1235 - accuracy: 0.9996\n",
      "Epoch 397: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.1235 - accuracy: 0.9996 - val_loss: 0.3064 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 398/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1243 - accuracy: 0.9994\n",
      "Epoch 398: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 5s 71ms/step - loss: 0.1243 - accuracy: 0.9994 - val_loss: 0.3069 - val_accuracy: 0.9702 - lr: 1.0000e-07\n",
      "Epoch 399/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1230 - accuracy: 1.0000\n",
      "Epoch 399: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.1266 - accuracy: 0.9994 - val_loss: 0.3072 - val_accuracy: 0.9693 - lr: 1.0000e-07\n",
      "Epoch 400/400\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.1238 - accuracy: 0.9998\n",
      "Epoch 400: val_loss did not improve from 0.30512\n",
      "74/74 [==============================] - 3s 46ms/step - loss: 0.1260 - accuracy: 0.9994 - val_loss: 0.3073 - val_accuracy: 0.9702 - lr: 1.0000e-07\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history=model.fit(x_train, y_train, batch_size=64, epochs=400, validation_data=(x_test, y_test), callbacks=[rlrp, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T18:30:43.559722Z",
     "iopub.status.busy": "2024-08-14T18:30:43.559392Z",
     "iopub.status.idle": "2024-08-14T18:30:49.327818Z",
     "shell.execute_reply": "2024-08-14T18:30:49.326885Z",
     "shell.execute_reply.started": "2024-08-14T18:30:43.559696Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 2s 40ms/step - loss: 0.3073 - accuracy: 0.9702\n",
      "Accuracy of our model on test data :  97.01873660087585 %\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Accuracy of our model on test data : \" , model.evaluate(x_test,y_test)[1]*100 , \"%\")\n",
    "\n",
    "epochs = [i for i in range(400)]\n",
    "fig , ax = plt.subplots(1,2)\n",
    "train_acc = history.history['accuracy']\n",
    "train_loss = history.history['loss']\n",
    "test_acc = history.history['val_accuracy']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "fig.set_size_inches(20,6)\n",
    "ax[0].plot(epochs , train_loss , label = 'Training Loss')\n",
    "ax[0].plot(epochs , test_loss , label = 'Testing Loss')\n",
    "ax[0].set_title('Training & Testing Loss')\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel(\"Epochs\")\n",
    "\n",
    "ax[1].plot(epochs , train_acc , label = 'Training Accuracy')\n",
    "ax[1].plot(epochs , test_acc , label = 'Testing Accuracy')\n",
    "ax[1].set_title('Training & Testing Accuracy')\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel(\"Epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T18:30:49.329886Z",
     "iopub.status.busy": "2024-08-14T18:30:49.329505Z",
     "iopub.status.idle": "2024-08-14T18:30:49.347534Z",
     "shell.execute_reply": "2024-08-14T18:30:49.346517Z",
     "shell.execute_reply.started": "2024-08-14T18:30:49.329853Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>test_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.386238</td>\n",
       "      <td>2.521446</td>\n",
       "      <td>0.162692</td>\n",
       "      <td>5.934334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.480614</td>\n",
       "      <td>1.968244</td>\n",
       "      <td>0.191652</td>\n",
       "      <td>6.327982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.556881</td>\n",
       "      <td>1.739304</td>\n",
       "      <td>0.197615</td>\n",
       "      <td>10.093594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.635066</td>\n",
       "      <td>1.502396</td>\n",
       "      <td>0.362862</td>\n",
       "      <td>2.009143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.721133</td>\n",
       "      <td>1.282726</td>\n",
       "      <td>0.393526</td>\n",
       "      <td>4.945020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>0.999148</td>\n",
       "      <td>0.125761</td>\n",
       "      <td>0.970187</td>\n",
       "      <td>0.305800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>0.999574</td>\n",
       "      <td>0.123536</td>\n",
       "      <td>0.970187</td>\n",
       "      <td>0.306424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>0.999361</td>\n",
       "      <td>0.124295</td>\n",
       "      <td>0.970187</td>\n",
       "      <td>0.306897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>0.999361</td>\n",
       "      <td>0.126554</td>\n",
       "      <td>0.969336</td>\n",
       "      <td>0.307198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>0.999361</td>\n",
       "      <td>0.126033</td>\n",
       "      <td>0.970187</td>\n",
       "      <td>0.307341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Training Accuracy  Training Loss  test_acc  test_loss\n",
       "0             0.386238       2.521446  0.162692   5.934334\n",
       "1             0.480614       1.968244  0.191652   6.327982\n",
       "2             0.556881       1.739304  0.197615  10.093594\n",
       "3             0.635066       1.502396  0.362862   2.009143\n",
       "4             0.721133       1.282726  0.393526   4.945020\n",
       "..                 ...            ...       ...        ...\n",
       "395           0.999148       0.125761  0.970187   0.305800\n",
       "396           0.999574       0.123536  0.970187   0.306424\n",
       "397           0.999361       0.124295  0.970187   0.306897\n",
       "398           0.999361       0.126554  0.969336   0.307198\n",
       "399           0.999361       0.126033  0.970187   0.307341\n",
       "\n",
       "[400 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = pd.DataFrame(train_acc, columns=['Training Accuracy'])\n",
    "params['Training Loss'] = train_loss\n",
    "params['test_acc'] = test_acc\n",
    "params['test_loss'] = test_loss\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T18:30:49.349053Z",
     "iopub.status.busy": "2024-08-14T18:30:49.348763Z",
     "iopub.status.idle": "2024-08-14T18:30:49.363302Z",
     "shell.execute_reply": "2024-08-14T18:30:49.362345Z",
     "shell.execute_reply.started": "2024-08-14T18:30:49.349028Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "params.to_csv('Parameters.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T18:30:49.365382Z",
     "iopub.status.busy": "2024-08-14T18:30:49.364575Z",
     "iopub.status.idle": "2024-08-14T18:30:49.374252Z",
     "shell.execute_reply": "2024-08-14T18:30:49.373394Z",
     "shell.execute_reply.started": "2024-08-14T18:30:49.365350Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1626916527748108,\n",
       " 0.19165247678756714,\n",
       " 0.19761499762535095,\n",
       " 0.3628620207309723,\n",
       " 0.3935264050960541,\n",
       " 0.47614991664886475,\n",
       " 0.30749574303627014,\n",
       " 0.42930153012275696,\n",
       " 0.7504258751869202,\n",
       " 0.6882452964782715,\n",
       " 0.5468483567237854,\n",
       " 0.8194207549095154,\n",
       " 0.8219761252403259,\n",
       " 0.8671209812164307,\n",
       " 0.8321976065635681,\n",
       " 0.7802385091781616,\n",
       " 0.8390119075775146,\n",
       " 0.8500851988792419,\n",
       " 0.4267461597919464,\n",
       " 0.9480409026145935,\n",
       " 0.9437819123268127,\n",
       " 0.9165247082710266,\n",
       " 0.8994889259338379,\n",
       " 0.9054514765739441,\n",
       " 0.8458262085914612,\n",
       " 0.9258943796157837,\n",
       " 0.9071550369262695,\n",
       " 0.9114139676094055,\n",
       " 0.9403747916221619,\n",
       " 0.9267461895942688,\n",
       " 0.9565587639808655,\n",
       " 0.9557070136070251,\n",
       " 0.9344122409820557,\n",
       " 0.9471890926361084,\n",
       " 0.9557070136070251,\n",
       " 0.95485520362854,\n",
       " 0.9565587639808655,\n",
       " 0.9531516432762146,\n",
       " 0.9395229816436768,\n",
       " 0.9574105739593506,\n",
       " 0.9378194212913513,\n",
       " 0.9250425696372986,\n",
       " 0.9608176946640015,\n",
       " 0.9139693379402161,\n",
       " 0.9480409026145935,\n",
       " 0.959114134311676,\n",
       " 0.9557070136070251,\n",
       " 0.9395229816436768,\n",
       " 0.908858597278595,\n",
       " 0.8304940462112427,\n",
       " 0.9054514765739441,\n",
       " 0.9395229816436768,\n",
       " 0.9514480233192444,\n",
       " 0.9480409026145935,\n",
       " 0.9522998332977295,\n",
       " 0.9625213146209717,\n",
       " 0.963373064994812,\n",
       " 0.9625213146209717,\n",
       " 0.9608176946640015,\n",
       " 0.9667802453041077,\n",
       " 0.9608176946640015,\n",
       " 0.9557070136070251,\n",
       " 0.9540033936500549,\n",
       " 0.9650766849517822,\n",
       " 0.9667802453041077,\n",
       " 0.9684838056564331,\n",
       " 0.9650766849517822,\n",
       " 0.9667802453041077,\n",
       " 0.9642248749732971,\n",
       " 0.9667802453041077,\n",
       " 0.9608176946640015,\n",
       " 0.9642248749732971,\n",
       " 0.9642248749732971,\n",
       " 0.9659284353256226,\n",
       " 0.9667802453041077,\n",
       " 0.9684838056564331,\n",
       " 0.9684838056564331,\n",
       " 0.9701873660087585,\n",
       " 0.9693356156349182,\n",
       " 0.9676320552825928,\n",
       " 0.9684838056564331,\n",
       " 0.9650766849517822,\n",
       " 0.9667802453041077,\n",
       " 0.9684838056564331,\n",
       " 0.9684838056564331,\n",
       " 0.9684838056564331,\n",
       " 0.9693356156349182,\n",
       " 0.9684838056564331,\n",
       " 0.9676320552825928,\n",
       " 0.9684838056564331,\n",
       " 0.9684838056564331,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9676320552825928,\n",
       " 0.9684838056564331,\n",
       " 0.9701873660087585,\n",
       " 0.9693356156349182,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9701873660087585,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9701873660087585,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9701873660087585,\n",
       " 0.9693356156349182,\n",
       " 0.9701873660087585,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9701873660087585,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9701873660087585,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9701873660087585,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9701873660087585,\n",
       " 0.9693356156349182,\n",
       " 0.9701873660087585,\n",
       " 0.9693356156349182,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9693356156349182,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9701873660087585,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9701873660087585,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9684838056564331,\n",
       " 0.9701873660087585,\n",
       " 0.9693356156349182,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9701873660087585,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9701873660087585,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9693356156349182,\n",
       " 0.9701873660087585,\n",
       " 0.9693356156349182,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9701873660087585,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9693356156349182,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9693356156349182,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9693356156349182,\n",
       " 0.9701873660087585,\n",
       " 0.9693356156349182,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9693356156349182,\n",
       " 0.9693356156349182,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9701873660087585,\n",
       " 0.9693356156349182,\n",
       " 0.9701873660087585]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T18:30:49.376109Z",
     "iopub.status.busy": "2024-08-14T18:30:49.375414Z",
     "iopub.status.idle": "2024-08-14T18:30:51.692043Z",
     "shell.execute_reply": "2024-08-14T18:30:51.691057Z",
     "shell.execute_reply.started": "2024-08-14T18:30:49.376077Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 0s 8ms/step\n"
     ]
    }
   ],
   "source": [
    "# predicting on test data.\n",
    "pred_test = model.predict(x_test)\n",
    "y_pred = encoder.inverse_transform(pred_test)\n",
    "\n",
    "y_test = encoder.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T18:30:51.697605Z",
     "iopub.status.busy": "2024-08-14T18:30:51.697213Z",
     "iopub.status.idle": "2024-08-14T18:30:51.703604Z",
     "shell.execute_reply": "2024-08-14T18:30:51.702665Z",
     "shell.execute_reply.started": "2024-08-14T18:30:51.697564Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5],\n",
       "       [6],\n",
       "       [6],\n",
       "       ...,\n",
       "       [5],\n",
       "       [1],\n",
       "       [4]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T18:30:51.705537Z",
     "iopub.status.busy": "2024-08-14T18:30:51.704957Z",
     "iopub.status.idle": "2024-08-14T18:30:51.714878Z",
     "shell.execute_reply": "2024-08-14T18:30:51.713910Z",
     "shell.execute_reply.started": "2024-08-14T18:30:51.705502Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.5826364e-10, 3.8703857e-10, 1.0000000e+00, 6.0062216e-10,\n",
       "        2.5571686e-10],\n",
       "       [1.5157618e-19, 6.9359266e-14, 1.2832279e-14, 1.0000000e+00,\n",
       "        1.4091344e-15],\n",
       "       [1.3034091e-13, 8.5741908e-10, 3.8535650e-12, 1.0000000e+00,\n",
       "        7.3499171e-13],\n",
       "       ...,\n",
       "       [8.1969880e-08, 1.5685914e-07, 9.9999964e-01, 3.8304857e-09,\n",
       "        9.4555190e-08],\n",
       "       [9.9992549e-01, 6.6188361e-05, 4.1549597e-06, 3.5655323e-06,\n",
       "        6.5333819e-07],\n",
       "       [1.3779152e-08, 1.0000000e+00, 2.4622382e-09, 1.2202013e-09,\n",
       "        2.7584951e-10]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T18:30:51.716371Z",
     "iopub.status.busy": "2024-08-14T18:30:51.716012Z",
     "iopub.status.idle": "2024-08-14T18:30:51.733012Z",
     "shell.execute_reply": "2024-08-14T18:30:51.732043Z",
     "shell.execute_reply.started": "2024-08-14T18:30:51.716337Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Labels</th>\n",
       "      <th>Actual Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Predicted Labels  Actual Labels\n",
       "0                 5              5\n",
       "1                 6              6\n",
       "2                 6              6\n",
       "3                 4              4\n",
       "4                 5              5\n",
       "5                 7              7\n",
       "6                 6              6\n",
       "7                 5              5\n",
       "8                 6              6\n",
       "9                 5              5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])\n",
    "prediction['Predicted Labels'] = y_pred.flatten()\n",
    "prediction['Actual Labels'] = y_test.flatten()\n",
    "\n",
    "prediction.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T18:30:51.734446Z",
     "iopub.status.busy": "2024-08-14T18:30:51.734160Z",
     "iopub.status.idle": "2024-08-14T18:30:51.744602Z",
     "shell.execute_reply": "2024-08-14T18:30:51.743653Z",
     "shell.execute_reply.started": "2024-08-14T18:30:51.734423Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Labels</th>\n",
       "      <th>Actual Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1169</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1170</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1171</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1173</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1174 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Predicted Labels  Actual Labels\n",
       "0                    5              5\n",
       "1                    6              6\n",
       "2                    6              6\n",
       "3                    4              4\n",
       "4                    5              5\n",
       "...                ...            ...\n",
       "1169                 6              6\n",
       "1170                 5              5\n",
       "1171                 5              5\n",
       "1172                 1              1\n",
       "1173                 4              4\n",
       "\n",
       "[1174 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T18:30:51.746154Z",
     "iopub.status.busy": "2024-08-14T18:30:51.745864Z",
     "iopub.status.idle": "2024-08-14T18:30:52.547438Z",
     "shell.execute_reply": "2024-08-14T18:30:52.546613Z",
     "shell.execute_reply.started": "2024-08-14T18:30:51.746131Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with pd.ExcelWriter('Pred.xlsx', engine='openpyxl') as writer:\n",
    "    prediction.to_excel(writer, sheet_name=\"prediction\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T18:30:52.549785Z",
     "iopub.status.busy": "2024-08-14T18:30:52.548682Z",
     "iopub.status.idle": "2024-08-14T18:30:52.959373Z",
     "shell.execute_reply": "2024-08-14T18:30:52.958426Z",
     "shell.execute_reply.started": "2024-08-14T18:30:52.549747Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize = (12, 10))\n",
    "cm = pd.DataFrame(cm , index = [i for i in encoder.categories_] , columns = [i for i in encoder.categories_])\n",
    "sns.heatmap(cm, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')\n",
    "plt.title('Confusion Matrix', size=20)\n",
    "plt.xlabel('Predicted Labels', size=14)\n",
    "plt.ylabel('Actual Labels', size=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T18:30:52.961202Z",
     "iopub.status.busy": "2024-08-14T18:30:52.960832Z",
     "iopub.status.idle": "2024-08-14T18:30:52.968353Z",
     "shell.execute_reply": "2024-08-14T18:30:52.967516Z",
     "shell.execute_reply.started": "2024-08-14T18:30:52.961170Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cm.to_csv('Confusion_matrix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T18:30:52.969950Z",
     "iopub.status.busy": "2024-08-14T18:30:52.969655Z",
     "iopub.status.idle": "2024-08-14T18:30:52.999564Z",
     "shell.execute_reply": "2024-08-14T18:30:52.998680Z",
     "shell.execute_reply.started": "2024-08-14T18:30:52.969925Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.99      0.99      0.99       242\n",
      "           4       0.96      0.97      0.97       238\n",
      "           5       0.95      0.96      0.95       254\n",
      "           6       0.98      0.96      0.97       254\n",
      "           7       0.97      0.97      0.97       186\n",
      "\n",
      "    accuracy                           0.97      1174\n",
      "   macro avg       0.97      0.97      0.97      1174\n",
      "weighted avg       0.97      0.97      0.97      1174\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))\n",
    "report = classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T18:30:53.000988Z",
     "iopub.status.busy": "2024-08-14T18:30:53.000693Z",
     "iopub.status.idle": "2024-08-14T18:30:53.005464Z",
     "shell.execute_reply": "2024-08-14T18:30:53.004621Z",
     "shell.execute_reply.started": "2024-08-14T18:30:53.000962Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open('report.csv', 'w') as file:\n",
    "    file.write(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T18:30:53.007044Z",
     "iopub.status.busy": "2024-08-14T18:30:53.006739Z",
     "iopub.status.idle": "2024-08-14T18:30:53.251872Z",
     "shell.execute_reply": "2024-08-14T18:30:53.250889Z",
     "shell.execute_reply.started": "2024-08-14T18:30:53.007019Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save the Keras model\n",
    "model.save('saved_models/audio_classification_final.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T18:30:53.253278Z",
     "iopub.status.busy": "2024-08-14T18:30:53.253019Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\palsh\\AppData\\Local\\Temp\\tmp7u9ckery\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\palsh\\AppData\\Local\\Temp\\tmp7u9ckery\\assets\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "converter._experimental_lower_tensor_list_ops = False\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the converted model\n",
    "with open('saved_models/audio_classification_final.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5361692,
     "sourceId": 8915720,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5376921,
     "sourceId": 8936954,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5544338,
     "sourceId": 9174175,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
